
# Exercise week 2
Welcome to week 2! 
This is your 2nd mandatory exercise which must be completed until next week, i.e. 11.09.2023, 10:00. By that time please fill out the checklist on the [learnit page](https://learnit.itu.dk/course/view.php?id=3022225) to indicate which tasks you volunteer to present. 
You are very welcome to present incomplete solutions and describe what challenges you faced.
Please note that if not stated otherwise the programming exercises should be implemented by you, not by using a library. The intention is that you learn the transition from equations to code. Later in the course you are welcome to use programming frameworks. 

## Standard Regression Model Reminder

Standard regression model reminder. The standard linear model for regression is defined as follows
$$y_n=f(\mathbf{x}_n)=\mathbf{w}^T\mathbf{x}_n + \epsilon,\quad \epsilon \sim \mathcal{N}(0,\sigma^2),~\mathbf{w}=(w_0,w_1)^T.$$

For the regression tasks, please use the file points.txt which contains 20 samples for a 1D regression problem. It defines the training data: $\mathcal{X}=(x_n,y_n),~n=1,\ldots, N=20$. 
Given this 1D data, and assuming a parametric model, we can approximate the true outputs as follows $y_n\approx \widehat{y}_n=\widehat{w}_0+ \widehat{w}_1 x_n$. 

## Task 1: Programming - Bayesian Regression
Implement a the Bayesian Regression approach for 1D data using the data provided in points.txt. Divide the data randomly in training and test sets (50-50). 

a. Estimate $w_0 , w_1$ by Bayesian estimation as follows. 
	Assuming a Gaussian prior $p(\mathbf{w}) \sim\mathcal{N}(\mathbf{0},(1/\alpha) \mathbf{I})$ yields the posterior $p(\mathbf{w}|\mathcal{X}) \sim\mathcal{N}(\mathbf{\mu}_N, \Sigma_N)$, with
	 $$\mathbf{\mu}_N= \beta \Sigma_N \mathbf{X}^T\mathbf{y}, $$
	 $$\Sigma_N = (\alpha\mathbf{I} +\beta \mathbf{X}^T\mathbf{X})^{-1}, $$
	where $\mathbf{I}$ is the identity matrix, and $\mathbf{X}$ is the design matrix (= x values and column of 1). Set $\alpha=2$, $\beta=25$. To receive one estimate $\widehat{\mathbf{w}}=(\widehat{w}_0,~\widehat{w}_1)^T$, get one sample from the posterior $p(\mathbf{w}|\mathcal{X})$. 	

Hint: w = np.random.multivariate_normal(m_N.ravel(), S_N, num_w_samples).T 


b. Create $10$ different estimates for lines from (a) by sampling $\widehat{\mathbf{w}}$ to predict $y_n$. From these different lines, estimate the average line. Plot your results.</li>


c. Approximate the true datapoints $(x_n,y_n)$ by computing $\widehat{y}_n=\widehat{h}(x_n)$. </li>


d. Estimate the RMS error on the test set and compare it to the RMS error on the training set.
## Task 2: Programming Exercise - Bayesian Predictive Distribution

Replicate the Bayesian predictive distribution experiment shown in lecture. That is, assuming a flat prior, 

a. Create a sinusoidal dataset 
	 $$t(x) = \sin(2\pi x) + \chi,$$
	 where $x_i$, $i =1,\ldots,I$ are realisations of the random variable $X \sim \mathrm{Unif}([0 ,1])$
	 and
	 $\chi \sim N(0,0.3^2)$.	 


b. Create the design matrix $\Phi^\mathrm{T}$ corresponding to the nine radial basis functions 
	$$\phi_j(x) = \exp \left(-\frac{(x-\mu_j)^2}{2 s^2}\right).$$


c. Create a function that returns the predictive mean $y(x,\mathbf{m}_N)$ in the function of $x$ and the function that computes the 
	 predictive standard deviation $\sigma_N(x)$ in the function of x.  </li>


d. Plot the predictive mean $y(x,\mathbf{m}_N)$ and the confidence intervals $y(x,\mathbf{m}_N) \pm \sigma_N(x)$, together with the ground truth simulated observations, for $I=1,2,4,25$. 

## Task 3: Programming Exercise - Classification
The provided iris data consists of a total of 150 samples $\mathbf{x}_n \in \mathbb{R}^{4}$, $n=1,\ldots,150$, of three classes, where each data point has 4 dimensions. 


a. Read the data and the labels, and plot the data. (For reference: Each of the three classes has 50 samples each.) </li>

b. For each class, compute the sample mean $\widehat{\mathbf{\mu}}_k\in\mathbb{R}^4$, $k=1,\ldots, 3$
  $$\widehat{\mathbf{\mu}}_k = \frac{1}{N_k}\sum_{\mathbf{x}_n\in \mathcal{C}_k} \mathbf{x}_n, $$
  where $N_k=50$ is the number of samples for each class. 
    </li>


c. For each class, compute the sample covariance matrix $\widehat{\mathbf{\Sigma}}_k\in\mathbb{R}^{4\times 4}$, $k=1,\ldots, 3$ 
    $$\widehat{\mathbf{\Sigma}}_k = \frac{1}{N_k}\sum_{\mathbf{x}_n\in \mathcal{C}_k} \left(\mathbf{x}_n -\widehat{\mathbf{\mu}}_k\right)\left(\mathbf{x}_n -\widehat{\mathbf{\mu}}_k\right)^T $$
  </li>
 

 d. Compute the joint covariance matrix 
  $$\widehat{\mathbf{\Sigma}} = \sum_{k=1}^K \frac{N_k}{N}\widehat{\mathbf{\Sigma}}_k, $$
  where $N_k$ defines the number of samples of class $k$, and $N=150$ refers to the total number of samples.
    </li>


e. Compute the three class-specific discriminant functions 
    $$a_k\left(\mathbf{x}\right) = \mathbf{w}_k^T \mathbf{x} + w_{k0}, $$
    using the previous results from task (b), and task (d), by
    $$\mathbf{w}_k =\widehat{\mathbf{\Sigma}}^{-1} \widehat{\mathbf{\mu}}_k $$
    $$w_{k0} = \frac{1}{2} \widehat{\mathbf{\mu}}_k^T \widehat{\mathbf{\Sigma}}^{-1} \widehat{\mathbf{\mu}}_k +\ln(\mathbb{P}(\mathcal{C}_k)), $$
    where $\mathbb{P}(\mathcal{C}_k) = \frac{N_k}{N}$.
    </li>
 
 
 f. Assume each class is defined by a 4D Gaussian distribution $\mathcal{N}(\widehat{\mathbf{\mu}}_k,\widehat{\mathbf{\Sigma}})$. Get two new samples $\mathbf{x}_m^{(k)}\in\mathbb{R}^4$, $m=1,2$ for each class $k$.
	Hint: Check  https://numpy.org/doc/stable/reference/random/generated/numpy.random.multivariate_normal.html  </li>


g. Classify the new samples from the previous step (f) using the discriminant functions in step (e). This is done by computing $a_1(\mathbf{x}_m^{(k)})$, $a_2(\mathbf{x}_m^{(k)})$, and $a_3(\mathbf{x}_m^{(k)})$. Then assign the class $j$ by $j=\text{argmax}_{k} a_k$. <br/>
	Are the new samples $\mathbf{x}_m$ from step (f) correctly classified? 
	If not which classes are confused with one-another?
 </li>
</ol>
Additional non-mandatory task:
Select 2 of the 4 features and repeat steps (a)-(g) in the lower dimensional space. 


## Task 4: Theory
You are being handed a project where the task is to classify data samples into 2 classes, e.g. healthy vs. ill. 
Your colleague tells you excitedly that the model achieves 95\% accuracy. 
Upon further investigation you find that class 2 is always classified as class 1. 

How is this possible? 
Which metric(s) can be used instead of accuracy to capture this issue?
