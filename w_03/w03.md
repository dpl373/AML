# Exercise week 3

This is your 3rd mandatory exercise which must be completed until next week, i.e. 18.09.2023, 10:00. By that time please fill out the checklist on the [learnit page](https://learnit.itu.dk/course/view.php?id=3022225) to indicate which tasks you volunteer to present. 
You are very welcome to present incomplete solutions and describe what challenges you faced.
Please note that if not stated otherwise the programming exercises should be implemented by you, not by using a library. The intention is that you learn the transition from equations to code. Later in the course you are welcome to use programming frameworks. 


## Task 1: SVM - Theory and Practice 
NOTE: You are free to choose if you want to solve the with pen and paper only, or via some programming help, as in using wolfram alpha and/or python. If you choose to do the latter, you are still required to implement the equations. 
The XOR problem consists of four points $\mathbf{x}_n$, $n=1,\ldots,4$ from two classes, which are not linearly separable, as follows:
 

1. class 1: $\mathbf{x}_1=\begin{pmatrix} 1\\ 1 \end{pmatrix}$, $\mathbf{x}_2=\begin{pmatrix} -1\\ -1 \end{pmatrix}$, with labels $t_1=+1$, $t_2=+1$

2.class 2: $\mathbf{x}_3=\begin{pmatrix} -1\\ 1 \end{pmatrix}$, $\mathbf{x}_4=\begin{pmatrix} 1\\ -1 \end{pmatrix}$, with labels $t_3=-1$, $t_4=-1$.</li>


The goal of this exercise is to compute the discriminant
$$g(\mathbf{x}) = \mathbf{w}^T \mathbf{\phi}(\mathbf{x}),$$
which enables a linear classification in a higher dimension using the basis function $\mathbf{\phi}$.

a. Draw the points and plot them such that the two different classes can be distinguished (either by color and or marker). </li>

b. Since the points are not linearly separable in 2D, they will be transferred to a higher dimension in an attempt to make them linearly separable in a higher dimension. Use the following basis function to transfer each of the four 2D points to 6D: $\mathbf{\phi} :\mathbb{R}^2 \rightarrow \mathbb{R}^6$ 
	$$\mathbf{\phi}(\mathbf{x}) = \mathbf{\phi} (x_1,x_2) = \left( 1,~ \sqrt{2}~ x_1,~ \sqrt{2}~ x_2,~ \sqrt{2} ~x_1 x_2 ,~ x_1^2 ,~ x_2^2\right)^T,$$
	 i.e. calculate $\mathbf{z}_n=\mathbf{\phi}(\mathbf{x}_n)$, $n=1,\ldots,4=N$. 
	</li>

c. Use the known values to complete Eq. (7.10) from Bishop [PR] 
	$$\tilde{L}(\mathbf{a})=\tilde{L}\left(a_1,a_2,a_3,a_4 \right) =\sum\limits_{n=1}^4 a_n -\frac{1}{2} \sum\limits_{m=1}^4 \sum\limits_{n=1}^4 a_n a_m t_n t_m \mathbf{\phi}(\mathbf{x}_n)^T\mathbf{\phi}(\mathbf{x}_m),$$
	 where $k(\mathbf{x}_n,\mathbf{x}_m)=\mathbf{\phi}(\mathbf{x}_n)^T\mathbf{\phi}(\mathbf{x}_m)$. Note: The values $a_n$ are unknown at this point. A solution for this specific data set will be derived by you in task e. 
	</li> 

d. Compute the derivative of $\tilde{L}(\mathbf{a} )$ with respect to $a_k$, i.e. the four components of the gradient 
	$$\nabla \tilde{L}(\mathbf{a}) =  \begin{pmatrix} \frac{\partial}{\partial a_1} \tilde{L}\left(a_1,a_2,a_3,a_4 \right) \\				\frac{\partial}{\partial a_2} \tilde{L}\left(a_1,a_2,a_3,a_4 \right)\\				\frac{\partial}{\partial a_3} \tilde{L}\left(a_1,a_2,a_3,a_4 \right)\\				\frac{\partial}{\partial a_4} \tilde{L}\left(a_1,a_2,a_3,a_4 \right) \end{pmatrix} = \ldots  $$
	</li> 

e. Derive the equation system from $\nabla\tilde{L}(\mathbf{a})=0$ and solve for $\mathbf{a}=\left(a_1,a_2,a_3,a_4 \right)^T$. 
	Hint: One example to rewrite a 2D gradient of a function into a non-homogeneous equation system: $$\nabla f(\mathbf{x})= \begin{pmatrix} x_1+3x_2 + b_1\\2x_1-x_2+b_2\end{pmatrix} = \begin{pmatrix} 1 &3\\2& -1\end{pmatrix}\begin{pmatrix} x_1\\x_2\end{pmatrix}+ \begin{pmatrix} b_1\\ b_2 \end{pmatrix}= \mathbf{A}\mathbf{x}+\mathbf{b} \stackrel{!}{=} \mathbf{0}$$
	</li> 

f. Which of the four training points are support vectors? How do the values of $a_n$ answer this question? </li>

g. Now that all four values of $\mathbf{a}$ have been computed, employ Eq.(7.29) to compute $\mathbf{w}$:
	$$\mathbf{w} = \sum_{n=1}^4 a_n  t_n \mathbf{z}_n = \sum_{n=1}^4 a_n  t_n \mathbf{\phi}(\mathbf{x}_n) $$
	Please note: $\mathbf{w},\mathbf{z}_n\in\mathbb{R}^6$ and $t_n\in\mathbb{R}$.  </li>	

h. Give the discriminant function $g$ based on the original input space 
	 $$g(\mathbf{x}) = g(x_1,x_2)= \mathbf{w}^T \mathbf{\phi}(\mathbf{x})= \ldots$$
	</li>		

i. Apply the discriminant function and compute the resulting values for the training input samples $g(\mathbf{x}_i)$, $i=1,\ldots, 4$. How are they classified? Are they correctly classified? </li>
</ol>


## Task 2: Programming Exercise - kernel SVM
The goal is to implement and evaluate different kernels for SVMs for one dataset. 
For this programming exercise the notebook **w03_svm_sklearn.ipynb** (or optionally **w03_svm_tensorflow.ibynb** for students experienced in tensorflow) is provided, which should be used and adapted. 


a. Implement:

<li> a linear kernel </li>

<li> a radial basis function kernel</li> 

<li> a polynomial kernel</li> 
<


b.  Which of these performs best on the data, in terms of speed and quality? (Do not forget to set the random seed to receive reproducible results.)</li> 

c. Test different values of $c$ and $d$ for the polynomial kernel. Which of them work best?</li> 

d. Test different values of $\gamma$ for the RBF kernel. Which of them works best?</li> 

e. Change the part of the code which generates the data such that it becomes linearly separable.</li> 

f. Re-evaluate the three kernels. Do you get the same result?</li> 



## Task 3: Principal Component Algorithm (PCA)

The main objective of this task it to implement the Principal Component Analysis (PCA) yourself. Please look into [this additional file](pca.md) which provides you a ste-by-step description of how to implement the PCA. 

a. The steps are described in detail in the following. To test your implementation of the steps (1)-(6) on the bodyfat data, which originally has $D=15$ dimensions. Use your PCA implementation to perform dimensionality reduction from $D=15$ to $M=2$. Plot the resulting 2D datapoints $\mathbf{z}_n\in \mathbb{R}^2$. Note for clarity: you can use numpy for the eigendecomposition!</li> 

b. Perform dimensionality reduction via PCA using sklearn.decomposition.PCA. How do you have to set the input parameters?</li>

c. How can PCA be used to perform regression? </li>



## Task 4: PCA Face Model

The purpose of this task is to create a 3D face model from a set of 3D faces given as sets of 3D points. Please look into [this additional file](pca.md) for more guidance. 
We provide data in faces.mat in the data folder. 
The data was generated using the [Candide3 face model](https://people.isy.liu.se/icg/ahlberg/papers/reports/R-2326.html) which you can you to generate your own data, if you feel like it. 
Please check the data folder, where you can find candide.wrl, and candide3.mat. You might find one of the two formats more convenient for the data generation. 

Hint: To read mat-files in Python:
import scipy.io
mat = scipy.io.loadmat('file.mat')


a. Estimate a PCA for the data matrix $X\in\mathbb{R}^{D\times N}$. Ideally you should use your own implementation from the previous task, but if you don't have one, use a builtin solution to proceed with the following tasks. Here, you are being provided with $N=40$ samples, i.e. 3D faces, stored columnwise. Each face has a data dimension of $D=339$, i.e. $113$ 3D points per 3D face.</li>

b. Plot the mean face $\overline{\mathbf{x}}\in \mathbb{R}^{D\times 1}$, and the variants of the first three principal components $\mathbf{u}_i \in\mathbb{R}^{D\times 1}$, $i=1,2,3$ as faces, i.e.: remember to add the mean face to each $\mathbf{u}_i$.</li> 

c. Create 3 new faces, which are not part of the data by choosing a fixed number of components $M$, and vary the weights $\alpha_k$. 
	$$\mathbb{R}^{D\times 1} \ni \mathbf{f} = \overline{\mathbf{x}}+\sum\limits_{k=1}^M \alpha_k \mathbf{u}_k .$$
   </li>

d. Approximate a new face shape. Choose one of your created faces from task c, and pretend it is unknown, i.e. pretend that the weights $\alpha_k$ are unknown. (This would be the case for every new face, which is not part of the training data or has not been generated by you.) Then estimate the coefficients of the model with a lower dimension than before, i.e. choose $K < M$ and solve the following equation system for $\mathbf{\alpha}$: 
		$$\mathbb{R}^{D\times 1} \ni \mathbf{f}_\text{new}-\overline{\mathbf{x}} = \mathbf{U} \mathbf{\alpha} ,$$
  where $\mathbf{\alpha} = (\alpha_1,\ldots,\alpha_K)^T \in\mathbb{R}^K$, and $\mathbf{U}=[\mathbf{u_1},\ldots,\mathbf{u_K} ] \in \mathbb{R}^{D\times K}$. 
  Compute the approximated face $\mathbf{f}_{\text{new}}\approx\widehat{\mathbf{f}}=\overline{\mathbf{x}} + \mathbf{U} \mathbf{\alpha}$.  
	Visualise your result by plotting the true vs. estimated face. Compute the error as $||\widehat{\mathbf{f}}-\mathbf{f}||_2^2$. </li>

e.  How many components do you need to preserve at least 90% of the variance of the data? keyword: proportion of variances.</li> 
</ol>
  
Hint for visualisation: The file triangles.mat contains connective information for the face data points. It contains an array with 184 rows, where each row contains the point indices (starting at 1) of points which should be connected by a triangle. 

Hint for Task 4d: How to solve this equation system if $U$ is not square? In this case the equation system is overdetermined. Revisit the lecture material for linear regression, and you will find the solution. You will find that you can use the [pseudo inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) of $U$, as follows:

$$\mathbf{f}_\text{new}-\overline{\mathbf{x}} = \mathbf{U} \mathbf{\alpha}$$

$$\mathbf{U}^T (\mathbf{f}_\text{new}-\overline{\mathbf{x}}) = \mathbf{U}^T\mathbf{U} \mathbf{\alpha}$$

$$(\mathbf{U}^T\mathbf{U})^{-1}\mathbf{U}^T (\mathbf{f}_\text{new}-\overline{\mathbf{x}}) = \mathbf{\alpha}$$
