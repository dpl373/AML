# Exercise week 4

This is your 4th mandatory exercise which must be completed until next week, i.e. 25.09.2023, 10:00. By that time please fill out the checklist on the [learnit page](https://learnit.itu.dk/course/view.php?id=3022225) to indicate which tasks you volunteer to present. 
You are very welcome to present incomplete solutions and describe what challenges you faced.
Please note that if not stated otherwise the programming exercises should be implemented by you, not by using a library. The intention is that you learn the transition from equations to code. Later in the course you are welcome to use programming frameworks. 


## Task 1: EM Algorithm via kmeans
Please note that we are following the notation in the Bishop book, specifically 9.2. 
Assume data points $\mathbf{x}_n\in\mathbb{R}^D$ are given and you know the data is separated into $K$ clusters. Each cluster is represented by one cluster center $\mathbf{m}_k\in\mathbb{R}^{D}$, hence all of them can be gathered in the following matrix 
$$\mathbf{M} := [\mathbf{m}_1,\ldots, \mathbf{m}_K ] \in \mathbb{R}^{D\times K}. $$

Each point $\mathbf{x}_n \in \mathbb{R}^D$ is assigned to one cluster. 

$$r_{nk} := \begin{cases} 
1 & , ~ \mathbf{x}_n \text{ belongs to cluster }k\\\ 
0 & , ~ \text{otherwise} \end{cases}
$$ 

$$\mathbf{R} := \begin{pmatrix}  r_{11} & \ldots & r_{1K} \\\ \vdots && \vdots \\\ r_{N1} & \ldots & r_{NK} 
\end{pmatrix}\in \\{0,1 \\}^{N\times K},$$ 

where $r_{nk}\in \\{0,1 \\}$ are an indicator variables, which are one if $\mathbf{x}_n$ belongs to cluster $k$.  

The goal is to minimize:

$$J(\mathbf{M},\mathbf{R}) = \sum_{n=1}^N\sum_{k=1}^K r_{nk} ||\mathbf{x}_n-\mathbf{m}_k||_2^2. $$

by alternation, that is, by fixing one of the unknowns $\mathbf{m}\_{k}$ and $r\_{nk}$, and estimating the other. Derive this algorithm by minimising $J$ with respect to one variable at a time and show further how it leads to the following steps:

1. (E-step) Assigning each data point $\mathbf{x}_n$ to one cluster center per point. </li>	
2. (M-step) Estimating the cluster centers, i.e.  $\mathbf{m}_k \in \mathbb{R}^D$. </li>  
</ol>

Hints:
-  The variables $r_{nk}$ are discrete (binary with constraints)---how do you minimise a function with a discrete variable?
- The assignment can be performed independently for each data point $\mathbf{x}_n$. </li>
-  A cluster centre estimate does not depend on data assigned to another cluster. 
 </ul>
 </li>
	
## Task 2: EM and GMMs

(a) Implement the Expectation Maximisation algorithm for fitting a one-dimensional Gaussian mixture model to data points. Test your implementation with the Old Faithful Geyser Data set by considering only the duration of the eruption. 
The data is provided in the file "faithful.dat" in the data folder. 

(b) Try the fitting with different initialisations and visualise the estimation results. Show both an example when the algorithm converges to a good solution and when it fails. Can you explain why it fails sometimes?

Hint: You can find the algorithm "EM for Gaussian Mixtures" on page 438/439 in the Bishop book [PR]. Please note that this description is for multiple dimensions, while this task only requires 1D. 


## Task 3: Regression

Study the regression example on TensorFlow available [here](https://www.tensorflow.org/tutorials/keras/regression). Thereafter, use the following toy data set
$$\mathbf{x}=(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)$$
$$\mathbf{t}=(0.15, -0.16, -0.61, -0.86, -1.02, -0.44, 0.16, 0.05, 0.45, 1.39, 0.86)$$

(a) Fit a linear regression model (a line) to the points as explained in the tutorial. 

(b) Likewise, fit a MLP network to the data. Use one hidden layer with 10 neurons with the tanh nonlinearity and linear activation on the output layer. Let the model overfit to the data so that it interpolates the data points. Plot the regressed function on the interval $[0,1]$. 
