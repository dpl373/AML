{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3K-_yoeKvcpC"
   },
   "source": [
    "## Task 05.4: Implementation demo for feature visualisation and saliency maps\n",
    "\n",
    "ITU KSADMAL1KU - Advanced Machine Learning for Computer Science 2023\n",
    "\n",
    "by Stefan Heinrich, with material by Kevin Murphy.\n",
    "\n",
    "This notebook was in part co-developed with Mingbo Cai at Uni Tokyo.\n",
    "\n",
    "All info and static material: https://learnit.itu.dk/course/view.php?id=3022225\n",
    "\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBf5W49_vcpF"
   },
   "outputs": [],
   "source": [
    "# @title #### import dependencies\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm # colormap\n",
    "import random\n",
    "import ast\n",
    "import os\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiTTB5PHvcpG"
   },
   "outputs": [],
   "source": [
    "random.seed()\n",
    "cuda_id = random.randint(0,5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "print('Using device', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icocKKetvcpG"
   },
   "source": [
    "### Introduction: This demo illustrates some of the approaches for visualizing the features learned by neural networks and how they make decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lRz-MwevcpH"
   },
   "source": [
    "#### We use a 11-layer [VGG](https://arxiv.org/pdf/1409.1556.pdf) model trained on [ImageNet](http://www.image-net.org/) as an example.\n",
    "Pytorch provides several popular [models](https://pytorch.org/docs/stable/torchvision/models.html) with pre-trained parameters. Feel free to check them out after class.\n",
    "##### Take a look at the output in the next cell.\n",
    "**features** is a stack of layers that are applied to image input one after another, to extract more and more abstract features. You can use vgg11.features\\[:K\\] to extract part of it and apply to your input, this will yield feature after the K-1 layer \n",
    "\n",
    "**avgpool** is a pooling layer that will adapt its pooling size according to the input to ensure the output has fixed size (7x7). This is important because its output will be flattened and pass through fully-connected layers. Those fully-connected layers have fixed input dimensions, so we don't want the input to those layers change depending on the size of images.\n",
    "\n",
    "**classifier** is the final stack of layers that ultimately output a number for each possible category in the training data. Passing these numbers for each image sample through a softmax function will yield a vector with values in \\[0, 1\\] that sum to 1, offering the predicted probability of the image belonging to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dijQrWbmvcpH"
   },
   "outputs": [],
   "source": [
    "# @title #### Model\n",
    "\n",
    "vgg11 = models.vgg11(weights='IMAGENET1K_V1')\n",
    "vgg11.eval() # this indicates that we are using it for evaluation mode (not doing any training of the network)\n",
    "vgg11.to(device) # this commands moves the parameters onto the GPU so that you can run the model on GPU\n",
    "print(vgg11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivurhOkCvcpI"
   },
   "source": [
    "#### ImageNet dataset: all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kqazLOzvcpI"
   },
   "outputs": [],
   "source": [
    "!wget https://gist.githubusercontent.com/VishDev12/971f2835aa1adf2ad30495a25a45b1dc/raw/45ae5963783565e96698111677347a167f86c094/imagenet1000_clsidx_to_labels.txt\n",
    "imagenet_classes_file = 'imagenet1000_clsidx_to_labels.txt'\n",
    "with open(imagenet_classes_file, 'r') as file:\n",
    "    content = file.read()\n",
    "    imagenet_classes = ast.literal_eval(content)\n",
    "print('ImageNet classes:', imagenet_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnPdGES3vcpJ"
   },
   "source": [
    "##### Preprocessing\n",
    "We define the standard transformation of images that were used when VGG model was trained: the resolution, some center cropping, and normalisation. These transformations will be applied to the input images we want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPVLVs4MvcpJ"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225] )])\n",
    "# This normalization is the default processing of VGG network\n",
    "\n",
    "transform_no_normalize = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()])\n",
    "# We make another transformation without normalization just to visualize the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUNNpZuQvcpJ"
   },
   "source": [
    "#### Illustration of some sample images of one category (church) to illustrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izA4ia1iygMG"
   },
   "outputs": [],
   "source": [
    "!mkdir church\n",
    "!cd church\n",
    "!wget -P ./church https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/The_second_Catholic_Church_to_be_built_in_Montana.jpg/640px-The_second_Catholic_Church_to_be_built_in_Montana.jpg\n",
    "!wget -P ./church https://www.catholiceducation.org/en/images/Churchs/church-3481187_640.jpg\n",
    "!wget -P ./church https://cdn.pixabay.com/photo/2012/03/02/00/36/silhouette-20787_640.jpg\n",
    "!wget -P ./church https://cdn.pixabay.com/photo/2016/10/01/14/27/dom-1707664_640.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9XejNnqvcpK"
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "images_orig = []\n",
    "for file in os.listdir('./church'):\n",
    "    if file.endswith('jpg'):\n",
    "        \n",
    "        img = Image.open(r'./church/' + file).convert('RGB')\n",
    "        images.append(transform(img))\n",
    "        images_orig.append(transform_no_normalize(img))\n",
    "        \n",
    "images = torch.stack(images).to(device)\n",
    "# This command is important to move the images also to memory on GPU. Otherwise pytorch will complain.\n",
    "\n",
    "fig = plt.figure(figsize=(16,4))\n",
    "img_grid = make_grid(images_orig, nrow=4)\n",
    "# this is a torchvision utility to put images together into a grid for plotting\n",
    "\n",
    "plt.imshow(img_grid.detach().numpy().transpose(1,2,0))\n",
    "# the transpose command above re-arranges the order of dimensions of a tensor.\n",
    "# make_grid by default generates a tensor of channel x height x width. \n",
    "# But matplotlib assumes that red, green and blue channels are in the last dimension of a tensor.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dpfa7ZS6bIV2"
   },
   "outputs": [],
   "source": [
    "# @title ##### Pass the images through the network and see its best guess of the category\n",
    "\n",
    "logit = vgg11(images)\n",
    "# By simply passing the image as input, the network \n",
    "\n",
    "pred_id = logit.argmax(dim=1) # the class with the highest logit will have the highest probability\n",
    "print('prdicted category ID:', pred_id)\n",
    "\n",
    "pred_class = [imagenet_classes[class_id]  for class_id in pred_id.detach().cpu().numpy()]\n",
    "print('network classification:', pred_class)\n",
    "\n",
    "class_id = torch.mode(pred_id).values\n",
    "print('ID of the class:', class_id)\n",
    "# we will use these class ids below! Also, you may change this manually for task 05.4.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaMhh5hvvcpL"
   },
   "source": [
    "### **Attribution with Salicence Maps**: one approach of visualizing the locations of images that make the strongest contribution to the network's classification decision, is [Grad-CAM](https://arxiv.org/pdf/1610.02391.pdf), that we implement next\n",
    "\n",
    "##### Basic ideas:\n",
    "- For images in the same category, find features in a layer generally important for predicting this category\n",
    "\n",
    "> This is achieved by first calculating the gradient of the output layer unit for that category against all neurons in the layer to investigate, and average the gradients over space and image samples. If the gradient is positive, it means the stronger such features are, the stronger the model believes the image belongs to this category.\n",
    "\n",
    "- For each location in the feature map, calculate the overall contribution of the features in that location to the decision, based on the gradient calculated.\n",
    "\n",
    "> This is performed by multiplying the features in that location with the gradient, take a sum, and truncate any negative sum to 0 (the authors of the paper believe negative sum usually means the content at the location favors other categories)\n",
    "\n",
    "- Last, we can resample the contribution map to the original resolution of image, and superimpose it on the image to visualize how much each part contribute to the network's decision (where the network looks at to make its decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaeZDT2mvcpL"
   },
   "outputs": [],
   "source": [
    "# @title ##### The function extracts features from a layer of interest, calculate the average gradient on features, and the contribution map for further visualization.\n",
    "\n",
    "def feature_importance(model, images, category, layer):\n",
    "    features = model.features[:layer+1](images)\n",
    "    # This pass the image through the network until the chosen layer\n",
    "    # and return the feature map at that layer. The +1 is because\n",
    "    # python index starts from 0 and ends at the index below the chosen index\n",
    "    \n",
    "    features.retain_grad()\n",
    "    # Normally, activation inside neurons do not require gradient. (can you guess why?)\n",
    "    # So when calling backward(), gradient with respect to them are not kept.\n",
    "    # Therefore, here we explicitly require these features to retain gradient\n",
    "    \n",
    "    output_logits = model.classifier(torch.flatten(model.avgpool(model.features[layer+1:](features)), start_dim=1))\n",
    "    # calculate the output at final layer, which can be converted to probability of each class\n",
    "    \n",
    "    torch.mean(output_logits[:,category]).backward()\n",
    "    # calculate gradient with respect to any parameters (and the features we explicitly request to retain gradient)\n",
    "    # along which the output logit of the selected category can increase\n",
    "    \n",
    "    pooled_feature_gradient = torch.mean(features.grad, dim=[0, 2, 3])\n",
    "    # average the gradient spatially and over batch to estimate what features\n",
    "    # can move output positively\n",
    "    \n",
    "    heatmap = F.relu(torch.mean(features * pooled_feature_gradient[:, None, None], dim=1))\n",
    "    # weight features according to importance, and calculate the overall useful features\n",
    "    # for each location\n",
    "    \n",
    "    heatmap = heatmap / torch.amax(heatmap, dim=(1, 2))[:, None, None]\n",
    "    return heatmap, pooled_feature_gradient, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROO51lO7vcpL"
   },
   "outputs": [],
   "source": [
    "# @title ##### Now we analyze the layer right before the final pooling using the sample images. You can also try other layers\n",
    "\n",
    "layer_investigate = 19\n",
    "\n",
    "heatmap, pooled_feature_gradient, features = feature_importance(vgg11, images, class_id, layer_investigate)\n",
    "# Yatta! Now we can visualize both the contribution map in that layer, and blend the contribution map with the original image to see what regions the network relies on to call an image a church\n",
    "\n",
    "hms = heatmap.detach().cpu()[:,None,:,:]\n",
    "# heatmap for the three iamages. we show them in one grid at once\n",
    "\n",
    "plt.imshow(make_grid(hms,nrow=4).numpy().transpose(1,2,0), cmap='jet')\n",
    "plt.title('heatmap of feature importance in layer {}'.format(layer_investigate))\n",
    "plt.show()\n",
    "\n",
    "cmap = cm.get_cmap('jet')\n",
    "resized_heatmap = transforms.Resize(224)(heatmap.detach())\n",
    "resized_heatmap_color = cmap(resized_heatmap.cpu().numpy())[:,:,:,:3].transpose(0, 3, 1, 2)\n",
    "\n",
    "plt.imshow(hms[0,0,:,:], cmap='jet', interpolation='nearest')\n",
    "plt.show()\n",
    "plt.imshow(resized_heatmap[0,:,:].cpu().detach().numpy(), cmap='jet', interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "weight = resized_heatmap[:,None,:,:].cpu().numpy() * 0.8\n",
    "highlighted_images = torch.stack(images_orig) * (1 - weight) + resized_heatmap_color * weight\n",
    "img_grid = make_grid(highlighted_images, nrow=4)\n",
    "plt.figure(figsize=(18,12))\n",
    "plt.imshow(img_grid.numpy().transpose(1,2,0))\n",
    "plt.title('heatmap of contribution from original images')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ae2rtQ4LT3eQ"
   },
   "source": [
    "##### Looking into the *pooled_feature_gradient*\n",
    "\n",
    "So the first key outcome of looking at the gradients was to identify what input aspects maximised the output for the church class. As a second outcome, we can also inspect the feature gradients. The more positive ones would capture something more diagnostic about the churches. In the second step, we follow up on this hypothesis and visualise the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfX0CziFUiez"
   },
   "outputs": [],
   "source": [
    "plt.bar(range(pooled_feature_gradient.shape[0]), pooled_feature_gradient.cpu().numpy())\n",
    "plt.show()\n",
    "# It seems feature '325' is particularly positive and could be the most important for the classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRRX1mucYxvz"
   },
   "source": [
    "### **Feature Visualisation**: One common approach to analyze neural network is to visualize patterns that maximize responses of certain neurons. We have seen  [this good tutorial](https://distill.pub/2017/feature-visualization/) in the previous exercise.\n",
    "\n",
    "In short, instead of optimizing parameters as we do when training a network, here we optimize image input to maximize the response of neurons of interest. Here we maximize the features that jointly contributed positively to the network's decision for the example pictures we used.\n",
    "\n",
    "We first generate image with random color in all pixels. Then we set an objective that tries to increase the values of features that in our sample images contributed positively to the network's classification decision of \"church\". By back-propagating gradients, we can adjust the random image to improve this objective function. The end result will highlight features at those locations that helped the network's decision.\n",
    "\n",
    "Notice that in such approach, some regularization is often needed to make the learned image closer to natural image. The regularization we apply here is a smooth prior: adjacent pixels turn to not change colors too much.\n",
    "\n",
    "You can try to **switch to the other definition of the loss** term in the function *feature_visualize_update* below that is currently commented out. This is adapted from this [paper](https://arxiv.org/pdf/1412.0035v1.pdf). The idea is to find what information is preserved by the features in the layer of interest, by searching for images that generate similar representation to that of the sample image. Here, we additinally weight the representation by the contribution map.\n",
    "\n",
    "You can also try to **redefine the loss** term in the function *feature_visualize_update* below to implement other methods in the tutorial above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7NaBh-yYS1X"
   },
   "outputs": [],
   "source": [
    "# @title ##### The function extracts features from a layer of interest, calculate the average gradient on features, and the contribution map for further visualization.\n",
    "\n",
    "def feature_visualize_update(image, model, heatmap, feature_gradient, target_features, layer, optimizer, reg_weight=0.1):\n",
    "    optimizer.zero_grad()\n",
    "    feature = model.features[:layer+1](image)\n",
    "\n",
    "    loss = - torch.mean(feature * F.relu(target_features.grad * target_features).detach() * heatmap[:, None, :, :])\n",
    "    # This loss function moves the image along the directions to maximize responses of neurons\n",
    "    # that contribute positively to the classification output. \n",
    "    \n",
    "    # loss = torch.mean((feature - target_features.detach() * heatmap[:, None, :, :]) ** 2    )\n",
    "    # the loss function above aims to bring the representation of newly generated images\n",
    "    # close to the sample images. Try to comment it out and run again. And see what happens if heatmap is not multiplied   \n",
    "        \n",
    "    regularizer = torch.mean((image[:,:,1:,:] - image[:,:,:-1,:]) ** 2) + torch.mean((image[:,:,:,1:] - image[:,:,:,:-1]) ** 2)\n",
    "    \n",
    "    total_loss = loss + regularizer * reg_weight\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    # This optimization updates the image tensor only.\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_DsDu4rZc1d"
   },
   "outputs": [],
   "source": [
    "# @title ##### be aware: the next step will take quite a while!\n",
    "\n",
    "batch_size = len(images_orig)\n",
    "feature_vis = torch.randn(batch_size, 3, 224, 224, device=device, requires_grad=True)\n",
    "# feature_vis = torch.tensor(images, device=device, requires_grad=True)\n",
    "# You can also try initializing with the sample images\n",
    "\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.Adam([feature_vis], lr = learning_rate)\n",
    "\n",
    "for it in range(800):\n",
    "    feature_vis = feature_visualize_update(feature_vis, vgg11, heatmap.detach(), pooled_feature_gradient.detach(),\n",
    "                                           features, layer_investigate, optimizer, reg_weight=0.1)\n",
    "    if it % 100 == 0:\n",
    "        fig = plt.figure(figsize=(18,9))\n",
    "        img_grid = make_grid(feature_vis , normalize=True, nrow=4) # * resized_heatmap[:,None,:,:]\n",
    "        \n",
    "        plt.imshow(img_grid.detach().cpu().numpy().transpose(1,2,0))\n",
    "        plt.show()\n",
    "        \n",
    "    # Notice this is not an exact implementation of the Guided Grad-CAM in the original paper."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
