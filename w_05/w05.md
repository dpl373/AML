
# Exercise week 5

This is your 5th mandatory exercise which must be completed until next week, i.e. 02.10.2023, 10:00. By that time please fill out the checklist on the [learnit page](https://learnit.itu.dk/course/view.php?id=3022225) to indicate which tasks you volunteer to present. 
You are very welcome to present incomplete solutions and describe what challenges you faced.
Please note that if not stated otherwise the programming exercises should be implemented by you, not by using a library. The intention is that you learn the transition from equations to code. Later in the course you are welcome to use programming frameworks. 


## Task 1: Regression with regularisation

Let us revisit the last week regressino problem where you overfitted a neural network to the toy data set

$$\mathbf{x}=(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)$$

$$\mathbf{t}=(0.15, -0.16, -0.61, -0.86, -1.02, -0.44, 0.16, 0.05, 0.45, 1.39, 0.86)$$

Now, fit it a MLP network to the data by using one hidden layer with 10 neurons with the nonlinearity of your choice and linear activation on the output layer where you now regularise the model by

(a) Using early stopping regularisation, 

(b) Weight decay (quadratic penalty on the weights), 

(c) Drop out regularisation. 

Plot the regressed function on the interval $[0,1]$. 

The regression example on TensorFlow is available at [here](https://www.tensorflow.org/tutorials/keras/regression).


## Task 2: CNN training and testing with MNIST

Investigate the training of CNN using hte MNIST dataset by running the notebook *w05_cnn_mnist_pytorch.ipynb*

(a) What can you conclude from the results when you run the notebook?

(b) Play with the training parameters, can you see any change in the results? What would be a systematic way of setting the parameters?

(c) What does the tsne visualisation tell you about the problem? Why do the training and prediction look different? 


## Task 3: Understanding feature visualisation and saliency maps

In this task, we go through the excellent blog post [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/) by Chris Olah to understand what a network has learned about an image. In particular, we look **conceptually** into the features that the network learned for representing the aspects of images and how filtered parts of the input are attributed to an output class. We look into one implementation for these two steps in task 05.3.

- Refresh yourself on the https://distill.pub/2017/feature-visualization/ blog post that you might have studied before; Read through the https://distill.pub/2018/building-blocks/ block post.

- After choosing different input images on the top of the blog post, inspect the detectors, e.g. ears and noses for *Labrador retriever and tiger cat* and flowers for *vase and lemon*. What could these numbers next to the detector mean relative to each other?

- Look into what different features the network sees in the four different layers (thus explore the "semantic dictionary" that emerged for the image). How are the features different for the layers?

- Inspect the *spatial attribution (with saliency maps)* on the different layers. Focus on how the activation of the learned features or "visual dictionary" emerged. Simple question first: why are the attribution spaces different? What attribution did you find between the different layers (w.r.t. your input image)? How are the spatial positions different, which the different layers look at?

- In the final step, the blog post explores how factorising all the feature channels can provide meaningful insight into the decision process of the network. Have the identified neuron groups learned and localised features that are similar to what your brain would spot as important to detect the animals or objects in the image? Now think about data that the network was trained with (we'll read into ImageNet in task 05.4): Can you find features that the network is focussing on that are counter-intuitive and potentially too specific to the image or dataset?

	*Hint* the last section *The Space of Interpretability Interfaces* provides a summary and discussion about the techniques that are illustrated in the blog post. Write down and bring to class questions that remain open here.


## Task 4: Implementation demo for feature visualisation and saliency maps
For this task, we look into an **implementation** of using gradient information to visualise how the network has learned to represent features and to attribute them for an image w.r.t. an output class. The key aim is to look more closely into visualising patterns (features!) that some neurons represent and into the importance, or salience, of specific input patches for the output, this time with a prepared implementation via *w05_visualisation_saliencymaps_pytorch.ipynb* (or in [Colab](https://colab.research.google.com/drive/1DwfbC37TpLCWrqHrVcPt86zjabmca4zP)).

- If you had no chance to do so before, read up in more detail on the ImageNet dataset series: https://paperswithcode.com/dataset/imagenet ; Read through the notebook and followup the two explanation blocks (*Attribution with Salience Maps* and *Feature Visualisation*).

- Explore the saliency maps on other (i.e. lower) layers. Are other aspects of the images more salient for these layers? Can you come up with a hypothesis why?

- Try different images of different categories (e.g. get some images of the ImageNet dataset or just arbitrary images that match one of the trained classes). What input parts are most salient?

- Explore and visualise what patterns maximise the output for **other** classes that you find interesting (first for the church images, then for your own from the previous sub-task. The *imagenet_classes* dictionary can tell you the id for other classes. Did the network identify salient patterns in the images that, in fact, do resemble aspects of your chosen class?

- It has been shown that by adding particular small noise to an image, one can generate *adversarial* images that trick a neural network into confidently classifying them into a wrong category. Try if you can modify the existing code to achieve this!
    
  *Hint:* We no longer need to constrain the noise to be spatially smooth, but we want it to be small in general so that the perturbation to the original images is not perceivable by a human. The input to the network should be the perturbed image, not the perturbation itself.

- Optional expert task: discuss with your (study) group: Can make your project/model (more) interpretable by applying Attribution with Salience Maps and Feature Visualisation?
