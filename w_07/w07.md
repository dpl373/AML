
# Exercise week 7

This is your 6th mandatory exercise which must be completed until next week, i.e. 23.10.2023, 10:00. By that time please fill out the checklist on the [learnit page](https://learnit.itu.dk/course/view.php?id=3022225) to indicate which tasks you volunteer to present. 
You are very welcome to present incomplete solutions and describe what challenges you faced.
Please note that if not stated otherwise the programming exercises should be implemented by you, not by using a library. The intention is that you learn the transition from equations to code. Later in the course you are welcome to use programming frameworks.
<style type="text/css">
    ol { list-style-type: lower-alpha; }
</style>

## Task 1: Understanding Word Embeddings

In this task, we are learning word embeddings using skipgram with negative sampling. The goal is to understand the method as well as the characteristic of the resulting vector space.

Now, first of all, further explore learned word embeddings with Word2Vec via http://projector.tensorflow.org/

Next, reproduce how building embeddings from skip-grams with negative sampling works
via  *w07_skipgram_pytorch.ipynb* (or in [Colab](https://colab.research.google.com/drive/1jrljPlVNPRqE_GXwh5mC54VAOigtbvV5))

- *Hint*: Optionally check the tutorial https://www.tensorflow.org/text/guide/word_embeddings
1. Refresh yourself and understand: how the tokenisation is realised!
2. Try different embedding sizes. How do similar tokens compare? Test your own token examples as well!
3. Figure out and test different window sizes for the skipgrams. What is the impact on quality (e.g. similarity of tokens) vs effort (training speed and convergence)?


## Optional task 2: Explore Data Augmentation options and impact

Now with the interested students, we want to look into data augmentation functions in torchvision (takes about 15min), via 
via *w07_image_augmentation_torchvision.ipynb* (or in [Colab](https://colab.research.google.com/drive/1JDiy0PKlXdGGXdcxy_I3bgYOU4vPVcqd)).

* Read up on what transformation functions are implemented in torchvision and familiarise yourself with the arguments in using them.
* Implement data augmentation into one of the previous/upcoming tasks (e.g. from 05.2 or 07.3). Realise an augmentation strategy that extends the dataset (consistently) into double, quadruple, or octuple the size by using different transformation steps. Explore:
   - Which transformations have a notable impact on the performance?
   - Are particular transformation settings useful? 
   - How is the computation time affected?
* *Expert task*: Research into automated augmentation libraries.


## Task 3: Understand Finetuning

Here we look now into transfer learning, particularly fine-tuning a general model for a specific downstream task, via *w07_finetune_cnn_pytorch.ipynb* (or in [Colab](https://colab.research.google.com/drive/1_pec9VGuU8PIxbYNh7Ed3xvFFFa5o1O1)).

1. Retrace how the pre-trained model is used in the new task to clarify and explore:
   - What is the strategy for fine-tuning?
   - What is the effect (on test accuracy) of using other learning-rate settings (for the fc layer)?

2. Discuss and explore with your colleagues:
   - What other strategies could be sensible?
   - What is the effect of these strategies, both on: absolute-best-accuracy and convergence-time?


## Optional expert task 4: Explore transfer learning framework *SimCLR*

If you still have time and energy, then let's look into a tutorial on SimCLR. This tutorial is 3rd party/homebrew and not necessarily correct in every aspect, nevertheless, it should give you some good insight.

Thus either via https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/semisupervised_simclr.ipynb (Tensorflow) or via 
https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial17/SimCLR.ipynb (PyTorch) explore constrastive learning to understand:

- How are similarity and contrast enforced?</li>
- Did this tutorial raise any new questions? -> bring them up in the group tutorial!


## Task 5: Exploring Transformer Models

Spend half of your allocated exercise time to freely research into transformer models, thus the existing versions, applications, and their impact. This task is purposefully exploratory and broad, in order to give you a chance to grasp some details in your interest. 

On this basis, we invite you all to a plenum discussions, either in the exercise slot with Jeppe or **via LearnIT**. As a minimum, explore the material below individually but discuss with your study group. In task 07.6 (below), we look into the technical details of one approach for attention (thus the basis for transformer models).

#### Material:

- Read one of the key papers on Attention.
- Browse through the vast resources on transformer models to get a feeling for the variants and use cases in application. Below is just the tip of the iceberg.

*Hint:* Be a bit careful with random internet blogs, as the quality is not always good.

Vaswani et al. 2017, 'Attention is all you need':
- https://arxiv.org/abs/1706.03762
- https://github.com/tensorflow/tensor2tensor
- https://github.com/jadore801120/attention-is-all-you-need-pytorch

Distill.pub blog on attention:
- https://distill.pub/2016/augmented-rnns/#attentional-interfaces

Blog post on attention and transformers by Lilian Weng 
- https://lilianweng.github.io/posts/2018-06-24-attention/

Blog post by Jay Alammar on Transformers (illustrative but a bit sloppy)
- https://jalammar.github.io/illustrated-transformer/

Huggingface, corporate blog on transformer models:
- https://huggingface.co/docs/transformers/

Step between GLoVe and transformers by Peters et al. 2018:
- https://arxiv.org/abs/1802.05365
- https://github.com/flairNLP/flair (replicated by Humboldt University)

BERT:
- Devlin et al. + Google 2019+: https://arxiv.org/abs/2005.14165
- https://github.com/google-research/bert
- BERS QA with SQuAD browse learned examples: https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/

GPT (1, 2, 3) - Radford et al. + Open AI 2018+:
- https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
- https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
- https://github.com/openai/finetune-transformer-lm
- https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf
- https://github.com/openai/gpt-2
- https://arxiv.org/abs/2005.14165
- https://github.com/openai/gpt-3
- Generate text with gpt2: https://huggingface.co/distilgpt2

T5:
- Raffel et al. 2020: https://www.jmlr.org/papers/v21/20-074
- https://github.com/google-research/text-to-text-transfer-transformer

NLP Leaderboards:
- https://sites.research.google/xtreme
- https://super.gluebenchmark.com/leaderboard/

#### Suggested Discussion Questions (do you have more?):

- What were the key findings in the evolution of the attention score function?
- What is the scalability of different transformer models in training and deployment?
- What tasks have been solved well with transformer models, and where are the current key challenges?
- What specific decoder and encoder designs are suggested for different natural language processing, audio, and vision tasks?
- What could be a taxonomy of the many different (efficient) transformer architectures (w.r.t. the key properties)?
- What are key numeric and semantic metrics in the major benchmarks tests and leaderboards?


## Task 6: Understanding Attention in Encoder-Decoder Architectures

In today's first task, we look into the Bahdanau attention in encoder-decoder architecture based on two RNNs for sequence-to-sequence learning (neural machine translation), via *w07_attention_pytorch.ipynb* (or in [Colab](https://colab.research.google.com/drive/1gOPe5h8SpLK2eFB_tOzjlVkQNeqddB1C)).

The key goal is to understand how attention is used to abstract the entire input sequence into a context variable and how to produce an output sequence from this. The example notebook includes many steps, so focus on the questions below; otherwise, this might be way too much to take in.

1. Carefully go through the encoder and definitions.  
  - Which type of data (query, key, or values) is represented in the decoders' hidden states?
  - Which type of data is represented in the encoder's hidden states?
2. Modify the experiment to replace the additive attention-scoring function with the scaled dot-product. 
  - How does it influence training efficiency?</li>
3. Expand the attention visualisation with some labels and experiment with some other examples for *engs* and *fras*. 
  - Is the attention distribution sensible, despite the small data, and short training time?
  - What mapping (most salient token) of input and output tokens did you find?
