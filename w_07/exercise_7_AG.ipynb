{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a\n",
    "\n",
    "Skip-gram Model:\n",
    "\n",
    "1. The Skip-gram model aims to predict the context words for a given target word. For each word in the text, it considers a window of surrounding words as context.\n",
    "It trains a neural network to maximize the probability of the context words given the target word.\n",
    "Negative Sampling:\n",
    "\n",
    "2. Training the Skip-gram model on all negative examples (words that are not in the context) can be computationally expensive.\n",
    "Negative Sampling addresses this by randomly selecting a small number of negative examples (words not in the context) for each positive example (context word) during training.\n",
    "The objective is to maximize the probability of the positive examples and minimize the probability of the selected negative examples.\n",
    "Word Embeddings:\n",
    "\n",
    "3. The weights of the hidden layer of the neural network become the word embeddings.\n",
    "These embeddings capture semantic relationships between words, such that words with similar meanings or usage have similar vector representations.\n",
    "Training Process:\n",
    "\n",
    "4. For each word in the dataset, a context window is defined, and positive and negative examples are selected.\n",
    "The neural networkâ€™s parameters are updated to maximize the objective function, typically using stochastic gradient descent or a similar optimization algorithm.\n",
    "Over many iterations through the dataset, the word embeddings improve and capture more of the semantic relationships in the data.\n",
    "Hyperparameters:\n",
    "\n",
    "5. Key hyperparameters include the size of the embeddings, the size of the context window, and the number of negative samples to use.\n",
    "Adjusting these hyperparameters can significantly affect the quality and usability of the resulting embeddings.\n",
    "\n",
    "SGNS is a powerful, efficient method for learning high-quality word embeddings from large text corpora, making it a popular choice in many natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b\n",
    "Embed size - 50\n",
    "![Alt text](image.png)\n",
    "\n",
    "Embed size - 100\n",
    "![Alt text](image-2.png)\n",
    "\n",
    "Embed size - 250\n",
    "![Alt text](image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window size 2 - very fast\n",
    "![Alt text](image-4.png)\n",
    "![Alt text](image-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window size 5\n",
    "![Alt text](image-5.png)\n",
    "![Alt text](image-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "window size 20 - very slow\n",
    "![Alt text](image-6.png)\n",
    "![Alt text](image-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "\n",
    "\n",
    "1. Retrace how the pre-trained model is used in the new task to clarify and explore:\n",
    "   - What is the strategy for fine-tuning?\n",
    "      - change fully connected layer to match number of classes in the new task and use smaller learning rate\n",
    "   - What is the effect (on test accuracy) of using other learning-rate settings (for the fc layer)?\n",
    "      - A higher learning rate might speed up the training but can overshoot the optimal values, while a lower learning rate might lead to more precise convergence to the optimal values but may take longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
