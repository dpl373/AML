{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0IFPwsfpbmV"
   },
   "source": [
    "## Task 07.3: Understand Finetuning\n",
    "\n",
    "ITU KSADMAL1KU - Advanced Machine Learning for Computer Science 2023\n",
    "\n",
    "by Stefan Heinrich, with material by Kevin Murphy.\n",
    "\n",
    "This notebook is based on on sec 13.2 of\n",
    "http://d2l.ai/chapter_computer-vision/fine-tuning.html and further adaptations by Kevin Murphy in https://colab.research.google.com/github/probml/probml-notebooks/blob/master/notebooks-d2l/finetune_cnn_torch.ipynb.\n",
    "\n",
    "All info and static material: https://learnit.itu.dk/course/view.php?id=3022225\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "The notebook demonstrates fine-tuning a resnet image classifier (pre-trained on ImageNet) to classify hotdog vs not-hotdog. The target dataset consists of 2 classes (hotdog vs no hotdog), and has 1400 images of each. (This example is inspired by [Season 4, Episode 4 of the TV show Silicon Valley](https://www.youtube.com/watch?v=ACmydtFDTGs).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PpWm6dI4pZP6"
   },
   "source": [
    "# @title #### Import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(seed=1)\n",
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "\n",
    "!mkdir figures # for saving plots\n",
    "\n",
    "!pip install matplotlib_inline\n",
    "!wget https://raw.githubusercontent.com/d2l-ai/d2l-en/master/d2l/torch.py -q -O d2l.py\n",
    "import d2l"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beobDckXqG9C"
   },
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J-prFPhBp2-F"
   },
   "source": [
    "d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip',\n",
    "                          'fba480ffa8aa7e0febbb511d181409f899b9baa5')\n",
    "\n",
    "data_dir = d2l.download_extract('hotdog')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MutqC4zbqI5e"
   },
   "source": [
    "train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))\n",
    "test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71lUA0L2qhrw"
   },
   "source": [
    "We show the first 8 positive and last 8 negative images. We see the aspect ratio is quite different."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_PElt-7vqTFy"
   },
   "source": [
    "hotdogs = [train_imgs[i][0] for i in range(8)]\n",
    "not_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]\n",
    "d2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9qcGG2qqv7L"
   },
   "source": [
    "We use data augmentation at train and test time, as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8yZ4uHZ0qhOk"
   },
   "source": [
    "# We specify the mean and variance of the three RGB channels to normalize the\n",
    "# image channel\n",
    "normalize = torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                             [0.229, 0.224, 0.225])\n",
    "\n",
    "train_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(), normalize])\n",
    "\n",
    "test_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(), normalize])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Hint*: Here you can also try augmentation mechanisms as illustrated in task 07.2"
   ],
   "metadata": {
    "id": "QJV9z8zumstB"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8lQbjlOq6OD"
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D7Qm9CIuq1HF"
   },
   "source": [
    "pretrained_net = torchvision.models.resnet18(pretrained=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GthEEQXJrJGF"
   },
   "source": [
    "The final layer is called `fc`, for fully connected."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MVZqwgf8q7W2"
   },
   "source": [
    "finetune_net = torchvision.models.resnet18(pretrained=True)\n",
    "finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)\n",
    "nn.init.xavier_uniform_(finetune_net.fc.weight)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAjTstlzrYm7"
   },
   "source": [
    "#### Fine tuning\n",
    "\n",
    "In D2L, they call their training routine `train_ch13`, since it is in their chapter 13. We modify their code so it uses a single GPU, by commenting out the `DataParallel` part."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eVZIYEgtrqDK"
   },
   "source": [
    "def train_batch(net, X, y, loss, trainer, devices):\n",
    "    X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum\n",
    "\n",
    "\n",
    "def train(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus()):\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    #net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    net = net.to(devices[0])\n",
    "    for epoch in range(num_epochs):\n",
    "        # Store training_loss, training_accuracy, num_examples, num_features\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch(net, features, labels, loss, trainer,\n",
    "                                      devices)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(\n",
    "                    epoch + (i + 1) / num_batches,\n",
    "                    (metric[0] / metric[2], metric[1] / metric[3], None))\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
    "          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
    "          f'{str(devices)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdXqBc2BvBpi"
   },
   "source": [
    "We update all the parameters, but use a 10x larger learning rate for the fc layer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tfowTobdrIg1"
   },
   "source": [
    "def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,\n",
    "                      param_group=True):\n",
    "    train_iter = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'),\n",
    "                                         transform=train_augs),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    test_iter = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'),\n",
    "                                         transform=test_augs),\n",
    "        batch_size=batch_size)\n",
    "    devices = d2l.try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    if param_group:\n",
    "        params_1x = [\n",
    "            param for name, param in net.named_parameters()\n",
    "            if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        trainer = torch.optim.SGD([\n",
    "          {'params': params_1x}, \n",
    "            {'params': net.fc.parameters(),'lr': learning_rate * 10}\n",
    "                ],\n",
    "                 lr=learning_rate, weight_decay=0.001)\n",
    "    else:\n",
    "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
    "                                  weight_decay=0.001)\n",
    "    train(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "                   devices)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "auUtFSrCrdhp"
   },
   "source": [
    "train_fine_tuning(finetune_net, 5e-5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbiKurEVwUT2"
   },
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qyvA-SYPxaQQ"
   },
   "source": [
    "net = finetune_net.to('cpu')\n",
    "net.eval(); # set to eval mode (not training)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mV3ZFW73wvAk"
   },
   "source": [
    "fname = os.path.join(data_dir, 'test', 'hotdog', '1000.png')\n",
    "from PIL import Image\n",
    "img = Image.open(fname)\n",
    "display(img)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E9EXslG3wGSg"
   },
   "source": [
    "img_t = test_augs(img) # convert to tensor\n",
    "batch_t = torch.unsqueeze(img_t, 0)\n",
    "out = net(batch_t)\n",
    "probs = F.softmax(out,dim=1)\n",
    "print(probs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cEEPj3qDxnk8"
   },
   "source": [
    "fname = os.path.join(data_dir, 'test', 'not-hotdog', '1000.png')\n",
    "from PIL import Image\n",
    "img = Image.open(fname)\n",
    "display(img)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H19-FUykxuFR"
   },
   "source": [
    "img_t = test_augs(img) # convert to tensor\n",
    "batch_t = torch.unsqueeze(img_t, 0)\n",
    "out = net(batch_t)\n",
    "probs = F.softmax(out,dim=1)\n",
    "print(probs)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
