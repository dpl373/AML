{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "So99OQumzo3M",
    "cEgqoIV75uUn",
    "oI06SGEA3sGa",
    "_8EJ5h852F9t"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4C5Ct9yoZKYa"
   },
   "source": [
    "## Optional expert task 09.3: Generalisation of Convolution in Graph Neural Networks\n",
    "\n",
    "ITU KSADMAL1KU - Advanced Machine Learning for Computer Science 2023\n",
    "\n",
    "by Stefan Heinrich, with material by Kevin Murphy.\n",
    "\n",
    "This notebook is based on material by Emanuele Rodol√†, Luca Moschella, and Antonio Norelli.\n",
    "\n",
    "All info and static material: https://learnit.itu.dk/course/view.php?id=3022225\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "*Note: the notebook includes a god amount of reading material code! You are not supposed to understand it all, but follow the questions from the tasks and the inline hints closely.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5CApnc6X4bFq",
    "cellView": "form"
   },
   "source": [
    "# @title #### import dependencies\n",
    "from __future__ import print_function, division\n",
    "\n",
    "!sudo pip install python-igraph\n",
    "!sudo pip install plotly\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from typing import Mapping, Union, Optional, List\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import igraph\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import trange"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T16:39:51.533229Z",
     "start_time": "2020-05-06T16:39:51.515048Z"
    },
    "id": "2tGN_bJOcfd3"
   },
   "source": [
    "# @title #### for reproducibility\n",
    "# Whenever we use randomness, we should make sure that our results are still reproducible, so we use fixed random seeds.\n",
    "\n",
    "import random\n",
    "np.random.seed(4)\n",
    "random.seed(4)\n",
    "\n",
    "torch.cuda.manual_seed(4)\n",
    "torch.manual_seed(4)\n",
    "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So99OQumzo3M"
   },
   "source": [
    "### Introduction: Digesting *a priori* the information in the structure "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have already learned how to consider the structure information in a simple case; when the domain $\\Omega$ is the Euclidean $\\mathbb{R}^2$, i.e. when the relations between variables are represented by a 2-dimensional grid, such as the pixels of an image.\n",
    "\n",
    "![immagine griglia 2d](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/09/pics/euclidean.PNG) \n",
    "\n",
    "The filters of a convolutional neural network elaborate pixels considering only their neighbors and are applied with the same weights all over the image. In this way **CNNs are digesting *a priori* the structure information**, using the properties of the domain -- the translational invariance and the locality of neighbours -- to **reduce the free parameters of the model**. This leads to a crucial speed-up of the training process and allows larger and more powerful models. \n",
    "\n",
    "CNNs can be naturally extended to general Euclidean domains $\\mathbb{R}^n$, but what can we do when $\\Omega \\neq \\mathbb{R}^n$?\n",
    "\n",
    "In many different fields such as Biology, Physics, Social Sciences and Computer Graphics we have to process signals defined on non-Euclidean domains, such as Graphs $\\Omega = G(\\mathcal{V, E})$ or [Manifolds](https://en.wikipedia.org/wiki/Manifold) $\\Omega = \\mathcal{X}$, where $\\mathcal{V} = \\left\\{1,2,\\dots,n\\right\\}$ are vertices or nodes and $\\mathcal{E}\\subseteq \\mathcal{V}\\times \\mathcal{V}$ are edges or connections.\n",
    "\n",
    "We would like to come out with a solution analoguous to CNNs; digesting *a priori* the structure information to reduce the free parameters proved to be very convenient in a learning setting based on a gradient descent optimization.\n"
   ],
   "metadata": {
    "id": "ywEzhCQP5WOy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Representing non-Euclidean data in an Euclidean memory\n",
    "\n",
    "Working with non-Euclidean domains provides a further challenge in representing the data. \n",
    "\n",
    "In the Euclidean setting, encoding the data in ordered matrices, vectors or tensors is so natural and effective that we do not even think to alternatives, and indeed the computer memory structure is itself Euclidean.\n",
    "\n",
    "In the non-Euclidean setting we have many alternatives, consider for instance a manifold $\\mathcal{X}$. It can be represented by a triangle mesh with vertices, edges and faces $\\mathcal{V,E,F}$, by a n-polygonal mesh, where we admit also non-triangle faces, by a simple point cloud or by a subdivision surface. \n",
    "\n",
    "And even when we have chosen the representation, we still have to come out with a last encoding procedure to store our data in matrices and tensors as required by the Euclidean structure of the physical memory in our computers. Is it always going to be this way? Maybe one day we will have new pieces of hardware structured as graphs, like human brains.\n"
   ],
   "metadata": {
    "id": "szHcT8dR5JJF"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEgqoIV75uUn"
   },
   "source": [
    "#### More background: Generalizing the convolution operation\n",
    "\n",
    "In this section we will explore the Spectral convolution, a generalization of the convolution operation to non-Euclidean domains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x08FyV945uUo"
   },
   "source": [
    "Given two functions $f, g: [-\\pi, \\pi] \\rightarrow \\mathbb{R}$ their convolution is defined as:\n",
    "\n",
    "  $$\n",
    "  (f\\star g)(x) = \\int_{-\\pi}^\\pi f(x') g(x-x') dx' \n",
    "  $$\n",
    "\n",
    "Convolution is a linear operation, so in the discrete case can be written as a matrix multiplication.\n",
    "\n",
    "\n",
    "The convolution of two vectors $\\mathbf{f} = (f_1, \\cdots, f_{n})^\\top$ and $\\mathbf{g} = (g_1, \\cdots, g_{n})^\\top$ is:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{f}\\star \\mathbf{g}\n",
    "=\n",
    "\\mathbf{G} \\mathbf{f} =\n",
    "\\begin{pmatrix}\n",
    "g_{1} & g_2 & \\cdots & \\cdots & g_{n} \\\\\n",
    "g_{n} & g_1 & g_2 & \\cdots & g_{n-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "g_{3} & g_{4} &  \\cdots &g_1 & g_2 \\\\\n",
    "g_{2} & g_{3} &  \\cdots &\\cdots & g_1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "f_1\\\\\n",
    "\\vdots\\\\\n",
    "f_n\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Notice that this is a circulant matrix, i.e. each row vector is rotated one element to the right relative to the preceding row vector. \n",
    "\n",
    "If a linear operator admits an eigendecomposition, we can express it in the basis of its eigenvectors $V = \\{v_1, \\cdots, v_n\\}$ through a diagonal matrix.\n",
    "\n",
    "It turns out that all circulant matrices are diagonalized by the same basis $\\Phi_\\mathcal{E} = \\{\\phi_1, \\cdots, \\phi_n\\}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{G}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "g_{1} & g_2 & \\cdots & \\cdots & g_{n} \\\\\n",
    "g_{n} & g_1 & g_2 & \\cdots & g_{n-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
    "g_{3} & g_{4} &  \\cdots &g_1 & g_2 \\\\\n",
    "g_{2} & g_{3} &  \\cdots &\\cdots & g_1 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\Phi_\\mathcal{E}\n",
    "\\begin{pmatrix}\n",
    "\\hat{g_1} & &\\\\\n",
    "& \\ddots &\\\\\n",
    "& & \\hat{g_n}\\\\\n",
    "\\end{pmatrix}\n",
    "\\Phi^\\top_\\mathcal{E}\n",
    "$$\n",
    "\n",
    "This basis $\\Phi_\\mathcal{E}$ is very special, it is the discretized Fourier basis in the Euclidean domain:\n",
    "\n",
    "$$\n",
    "\\Phi_\\mathcal{E} = \\{\\phi_1, \\cdots, \\phi_n\\}\n",
    "\\;\\;\\;\\;\\;\\;\n",
    "\\text{with}\n",
    "\\;\\;\\;\\;\\;\\;\n",
    "\\phi_k = \n",
    "\\begin{pmatrix}\n",
    "w_n^{0k} \\\\\n",
    "w_n^{1k} \\\\\n",
    "w_n^{2k} \\\\\n",
    "\\vdots \\\\\n",
    "w_n^{(n-1)k}\n",
    "\\end{pmatrix}\n",
    "\\;\\;\\;\\;\\;\\;\n",
    "\\text{and}\n",
    "\\;\\;\\;\\;\\;\\;\n",
    "w_n^{jk}=e^{\\frac{2 \\pi i}{n}jk}\n",
    "$$\n",
    "\n",
    "The expression of $\\mathbf{G}$ as $\\Phi_\\mathcal{E} \\mathbf{\\hat{G}} \\Phi^\\top_\\mathcal{E}$ will be our bridge towards non-Euclidean domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaAnQcFP5uUo"
   },
   "source": [
    " \n",
    "\n",
    "Infact we know a **generalization of the Fourier basis to graphs and manifolds**, the eigenvectors $\\Phi$ of the Laplacian operator:\n",
    "\n",
    "$$\\Delta = \\Phi \\Lambda \\Phi^\\top$$ \n",
    "\n",
    "where $\\Lambda$ is the diagonal matrix containing the eigenvalues of the Laplacian.\n",
    "\n",
    "![fourier laplacian](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/09/pics/fourier.png) \n",
    "\n",
    "On these non-Euclidean domains, the idea is to calculate the eigenvectors of the Laplacian in the first place, which constitutes the generalized Fourier basis $\\Phi$, and then **define the convolution operator** as:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}\n",
    "=\n",
    "\\Phi\n",
    "\\begin{pmatrix}\n",
    "\\hat{w_1} & &\\\\\n",
    "& \\ddots &\\\\\n",
    "& & \\hat{w_n}\\\\\n",
    "\\end{pmatrix}\n",
    "\\Phi^\\top\n",
    "$$\n",
    "\n",
    "Where $\\hat{w_i}$ are learnable parameters.\n",
    "\n",
    "Notice that in the Euclidean case this expression coincides with the standard convolution defined above, since the eigenvectors of the Laplacian in that case *are* the Euclidean Fourer basis. This is a desired property.\n",
    "\n",
    "However we have several drawbacks:\n",
    "- The coefficients $\\hat{w_i}$ depend on the basis $\\Phi$. So learned filters do not generalize across domains; the addition of a single node in a graph or the small differences in a mesh after a change of pose fatally changes the basis $\\Phi$. For instance a convolutional filter with parameters $\\hat{w_i}$ tuned to spot edges changes completely behaviour on a slightly different domain.\n",
    "\n",
    "![conv_broken](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/09/pics/broken_filter.png) \n",
    "- The number of trainable parameters per filter depends on $n$, the size of the domain. We want a convolutional filter with a fixed number of parameters like in the Euclidean case.\n",
    "- Since the trainable parameters $\\hat{w_i}$ are not properly constrained, there is a high chance that the learned filter is not localized in space, as seen in the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A1txf5W5uUp"
   },
   "source": [
    "To address these problems we put some constraints on the matrix $\\mathbf{\\hat{W}}$, parametrizing it in a different way.\n",
    "\n",
    "Instead of having a degree of liberty per element of the diagonal ($n$ learnable parameters), we substitute $\\mathbf{\\hat{W}}$ with the fixed eigenvalues of the Laplacian $\\Lambda$ altered by a single parametrized transformation $\\tau_\\alpha(\\lambda)$, which depends on a fixed number of learnable parameters $\\alpha$. \n",
    "Our new $\\mathbf{W}$ will be:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}\n",
    "=\n",
    "\\Phi\n",
    "\\begin{pmatrix}\n",
    "\\tau_\\alpha(\\lambda_1) & &\\\\\n",
    "& \\ddots &\\\\\n",
    "& & \\tau_\\alpha(\\lambda_n)\\\\\n",
    "\\end{pmatrix}\n",
    "\\Phi^\\top\n",
    "$$\n",
    "\n",
    "In the second part of this tutorial we will explore an application of this generalized convolution operation on graphs. We will define the Laplacian operator on graphs, then we will compute its eigendecomposition and define a proper transformation $\\tau_\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWj6V6Xf6QIp"
   },
   "source": [
    "### GCN & CORA\n",
    "\n",
    "The code in the following sections comes mainly from [this repository](https://github.com/tkipf/pygcn) and it is inspired by the paper [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907), where Thomas Kipf presents the Graph Convolutional Networks in PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbXcRM8A6QIq"
   },
   "source": [
    "#### The CORA dataset\n",
    "\n",
    "The CORA dataset is a much bigger dataset when compared to the Karate graph, the task is again node classification.\n",
    "\n",
    "In the CORA graph where:\n",
    "- Each **node** is a Machine Learning paper. \n",
    "- An **edge** represents one citation from one paper to another.\n",
    "- Each node is classified into one of seven possible Machine Learning sub-fields:\n",
    " - Case Based\n",
    " - Genetic Algorithms\n",
    " - Neural Networks\n",
    " - Probabilistic Methods\n",
    " - Reinforcement Learning\n",
    " - Rule Learning\n",
    " - Theory\n",
    "\n",
    "We will use a subset of the CORA dataset, preprocessed as suggested by [Thomas Kipf](https://github.com/tkipf/pygcn/tree/master/data/cora):\n",
    "\n",
    "- It considers only papers that are cited or cite at least once.\n",
    "- The words are [stemmed](https://en.wikipedia.org/wiki/Stemming)\n",
    "- Stopwords and infrequent words are removed.\n",
    "\n",
    "This subset contains **2708** paper with **1433** unique words.\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "[Here](http://networkrepository.com/cora.php) you can have fun exploring the complete CORA dataset, and many other graph datasets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "emlVrHLV6QIq"
   },
   "source": [
    "classes = [\n",
    "    'Case_Based',\n",
    "    'Genetic_Algorithms',\n",
    "    'Neural_Networks',\n",
    "    'Probabilistic_Methods',\n",
    "    'Reinforcement_Learning',\n",
    "    'Rule_Learning',\n",
    "    'Theory',\n",
    "]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjGopFHH6QIr"
   },
   "source": [
    "##### Dataset format"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zC_R1y1b6QIr"
   },
   "source": [
    "# The preprocessed CORA is contained in this repository under ./data/cora\n",
    "!git clone https://github.com/tkipf/pygcn.git"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgE_SE3m6QIs"
   },
   "source": [
    "The directory `pygnc/data/cora` contains two files: `cora.content` and `cora.cites`.\n",
    "\n",
    "The `cora.content` contains the description of each node. For each line it contains:\n",
    " - The id of the node.\n",
    " - The [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) text representation.\n",
    " - The label of that node."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kbNRPvH56QIs"
   },
   "source": [
    "import pandas\n",
    "headers = ['PaperID'] + [f'word{i}' for i in range(1433)] + ['label']\n",
    "pandas.read_csv('pygcn/data/cora/cora.content', sep=\"\\t\", names=headers)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6g6x3NO6QIs"
   },
   "source": [
    "The `cora.cites` contains the relationships between nodes. For each line it contains:\n",
    "\n",
    "- The first entry is id of the cited paper\n",
    "- The second entry id of the citing paper\n",
    "\n",
    "That is, the direction of the entry is right to left."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JNOiR05r6QIt"
   },
   "source": [
    "import pandas\n",
    "headers = ['Cited PaperID', 'Citing PaperID']\n",
    "pandas.read_csv('pygcn/data/cora/cora.cites', sep=\"\\t\", names=headers)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwtRs1TJ6QIt"
   },
   "source": [
    "##### Data loading\n",
    "\n",
    "The repository provides python functions to parse the preprocessed data.\n",
    "\n",
    "It returns the adjacency matrix, the node features, the labels for each node, the indices to split into train-test:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z-Xr6kwh6QIt"
   },
   "source": [
    "# Add the folder to the python path\n",
    "import sys\n",
    "sys.path.insert(0,'./pygcn/pygcn')\n",
    "\n",
    "from utils import load_data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path='pygcn/data/cora/')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-TZMCrrC6QIu"
   },
   "source": [
    "# As expected its shape is num_paper*num_paper\n",
    "adj.shape  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sa2BeEGC6QIu"
   },
   "source": [
    "# The adjacency matrix is... not just an adjacency matrix!\n",
    "# It is a normalized Laplacian matrix.\n",
    "# You can see the function used to build and normalize the Laplacian here:\n",
    "# https://github.com/tkipf/pygcn/blob/1600b5b748b3976413d1e307540ccc62605b4d6d/pygcn/utils.py#L56\n",
    "print(torch.sum(adj.to_dense(), 0))\n",
    "print(torch.sum(adj.to_dense(), 1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OMFZyDrz6QIu"
   },
   "source": [
    "# Each paper has 1433 features. The BOW representation of its text\n",
    "features.shape  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sE7fBjHfAQy8"
   },
   "source": [
    "#### Utility functions\n",
    "Ignore the methods in this section. The code is not necessary to unterstand for the tasks. They are just used for some nicer visualisations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "oje_dP1_6QIv"
   },
   "source": [
    "# @title ##### Model training\n",
    "\n",
    "def get_predictions(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    return correct\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    correct = get_predictions(output, labels)\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def plot_loss(losses):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(losses))),\n",
    "        y=losses,\n",
    "        # name=\"Name of Trace 1\"       # this sets its legend entry\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Train loss\",\n",
    "        xaxis_title=\"Epoch\",\n",
    "        yaxis_title=\"Loss\",\n",
    "        font=dict(\n",
    "            family=\"Courier New, monospace\",\n",
    "            size=18,\n",
    "            color=\"#7f7f7f\"\n",
    "        )\n",
    "    )\n",
    "    return fig\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title ##### Visualisations\n",
    "plt_glayt = 'fr'\n",
    "\n",
    "def refresh_bar(bar, desc):\n",
    "    bar.set_description(desc)\n",
    "    bar.refresh()\n",
    "\n",
    "def plot_graph(adj, node_colors, colors_legend = classes, title='CORA graph', \n",
    "               layt = None):\n",
    "    N = adj.shape[0]\n",
    "    adj = adj.coalesce()\n",
    "    edgeA, edgeB = adj.indices()[0, :], adj.indices()[1, :]\n",
    "    edgeA = edgeA.tolist()\n",
    "    edgeB = edgeB.tolist()\n",
    "\n",
    "    G = igraph.Graph.Adjacency((adj.to_dense() > 0).tolist())\n",
    "    if layt is None:\n",
    "        layt=G.layout(plt_glayt, dim=3)\n",
    "\n",
    "    Xn=[layt[k][0] for k in range(N)]# x-coordinates of nodes\n",
    "    Yn=[layt[k][1] for k in range(N)]# y-coordinates\n",
    "    Zn=[layt[k][2] for k in range(N)]# z-coordinates\n",
    "    Xe=[]\n",
    "    Ye=[]\n",
    "    Ze=[]\n",
    "    for e in zip(edgeA, edgeB):\n",
    "        Xe+=[layt[e[0]][0],layt[e[1]][0], None]# x-coordinates of edge ends\n",
    "        Ye+=[layt[e[0]][1],layt[e[1]][1], None]\n",
    "        Ze+=[layt[e[0]][2],layt[e[1]][2], None]\n",
    "\n",
    "    trace1=go.Scatter3d(x=Xe,\n",
    "                y=Ye,\n",
    "                z=Ze,\n",
    "                mode='lines',\n",
    "                line=dict(color='rgb(125,125,125)', width=1),\n",
    "                hoverinfo='none'\n",
    "                )\n",
    "\n",
    "    trace2=go.Scatter3d(x=Xn,\n",
    "                y=Yn,\n",
    "                z=Zn,\n",
    "                mode='markers',\n",
    "                name='actors',\n",
    "                marker=dict(symbol='circle',\n",
    "                                size=6,\n",
    "                                color=node_colors,\n",
    "                                colorscale='Viridis',\n",
    "                                line=dict(color='rgb(50,50,50)', width=0.5)\n",
    "                                ),\n",
    "                text=colors_legend,\n",
    "                hoverinfo='text'\n",
    "                )\n",
    "\n",
    "    axis=dict(showbackground=False,\n",
    "            showline=False,\n",
    "            zeroline=False,\n",
    "            showgrid=False,\n",
    "            showticklabels=False,\n",
    "            title=''\n",
    "            )\n",
    "\n",
    "    layout = go.Layout(\n",
    "            title=title,\n",
    "            width=800,\n",
    "            height=800,\n",
    "            showlegend=False,\n",
    "            scene=dict(\n",
    "                xaxis=dict(axis),\n",
    "                yaxis=dict(axis),\n",
    "                zaxis=dict(axis),\n",
    "            ),\n",
    "        margin=dict(\n",
    "            t=100\n",
    "        ),\n",
    "        hovermode='closest', \n",
    "        )\n",
    "\n",
    "    data=[trace1, trace2]\n",
    "    fig=go.Figure(data=data, layout=layout)\n",
    "    return fig, layt"
   ],
   "metadata": {
    "id": "QUZISsBS7WUA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckKB4j-X6QIv"
   },
   "source": [
    "#### CORA Graph\n",
    "\n",
    "Let's visualize the graph!\n",
    "\n",
    "As you can see, the graph has many disconnected compontents, many of which are very small. The preprocessing ensures that each component has at least 2 nodes.\n",
    "\n",
    "\n",
    "Note that in the visualization, nodes with the same colors have the same label.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nSFXw27d6QIv"
   },
   "source": [
    "fig, layt = plot_graph(adj, labels)\n",
    "fig"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN8Fdawr6QIv"
   },
   "source": [
    "#### MLP approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CA9rwk-b6QIv"
   },
   "source": [
    "The simplest approach is to use a Multi Layer Perceptron on the features of each node, independently.\n",
    "\n",
    "This means that we aim to predict the sub-field of each machine learning paper looking exclusively at its text, encoded in a BOW. We're not considering at all at the structure of the graph, i.e. the citations that link the papers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FiL6oFVs6QIw"
   },
   "source": [
    "def mlp_accuracy(model):\n",
    "    \"\"\"\n",
    "    Perfom a forward pass `y_pred = model(x)` and computes the accuracy\n",
    "    between `y_pred` and `y_true`\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_pred = model(features[idx_test])\n",
    "    acc = accuracy(y_pred, labels[idx_test])\n",
    "    print(f\"Accuracy: {acc:.5}\")\n",
    "    return acc"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SFw510j66QIw"
   },
   "source": [
    "# Model definition\n",
    "mlp = nn.Sequential(nn.Linear(1433, 500), \n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(500, 100),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(100, 7))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TBm-Iysh6QIw"
   },
   "source": [
    "print(\"Loss before training\")\n",
    "_ = mlp_accuracy(mlp)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHjyPJ1K6QIw"
   },
   "source": [
    "Cora graph visualization:\n",
    "- Yellow nodes: correct predictions\n",
    "- Blue nodes: wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CFeq41w86QIx"
   },
   "source": [
    "correct = get_predictions(mlp(features), labels)\n",
    "fig, layt = plot_graph(adj, correct, title=\"MLP performance before training\", layt=layt)\n",
    "fig\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6WJIY7I96QIx"
   },
   "source": [
    "opt = optim.Adam(mlp.parameters())\n",
    "\n",
    "losses = []\n",
    "mlp.train()\n",
    "\n",
    "for epoch in trange(500):\n",
    "    opt.zero_grad()\n",
    "    output = mlp(features[idx_train])\n",
    "    loss = F.cross_entropy(output, labels[idx_train])  # train only on the train samples\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plot_loss(losses)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jMNvNbhd6QIx"
   },
   "source": [
    "print(\"Loss after training\")\n",
    "accmlp = mlp_accuracy(mlp)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VGL-OAu6QIx"
   },
   "source": [
    "Cora graph visualization:\n",
    "- Yellow nodes: correct predictions\n",
    "- Blue nodes: wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P6jhRW0D6QIy"
   },
   "source": [
    "correct = get_predictions(mlp(features), labels)\n",
    "fig, layt = plot_graph(adj, correct, title=\"MLP performance after training\", layt=layt)\n",
    "fig\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyJoibaR6QIy"
   },
   "source": [
    "#### Graph convolutional network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZ-81QpZ6QIy"
   },
   "source": [
    "We can define an equivalent of the `nn.Layer` that uses the adjacency matrix in the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qD18g5FK6QIy"
   },
   "source": [
    "from torch.nn import Parameter\n",
    "\n",
    "import math\n",
    "class GraphConvolution(Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "\n",
    "        # A nn.Parameter is a normal tensor\n",
    "        # that is automatically registered as a model parameter\n",
    "        # so that it is inclued in `model.parameters()`.\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)  # sparse matrix multiplication\n",
    "        return output + self.bias"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBypjPr36QIy"
   },
   "source": [
    "-------------------------------------------------------------------------------\n",
    "#### **Tasks:**\n",
    "At first, let's truly understand the GCN model definition.\n",
    "\n",
    "* Did you notice anything strange in the forward pass?\n",
    "* Which is the shape of the adj matrix? Which is the shape of the output?\n",
    "\n",
    "Secondly, in contrast with this GCN, we can see that the MLP above, as expected, cannot learn much about the nodes that we didn't train on. Thus understand and discuss again:\n",
    "\n",
    "* Why was the GCN able to achieve this?\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "Ef8OIdf16QIz"
   },
   "source": [
    "#@title Solution üëÄ\n",
    "\n",
    "# We are performing the forward pass using the whole graph!\n",
    "# \n",
    "# We are not doing any batching, \n",
    "# since we need to perform a matrix multiplication \n",
    "# w.r.t the whole adjacency matrix!"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MA8Wf1xQ6QIz"
   },
   "source": [
    "These layers can be combined togheter to build complex models "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "59a6rTaN6QIz"
   },
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = self.gc2(x, adj)\n",
    "        return x"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nfs-bkd06QIz"
   },
   "source": [
    "def gcn_accuracy(model):\n",
    "    \"\"\"\n",
    "    Perfom a forward pass `y_pred = model(x)` and computes the accuracy\n",
    "    between `y_pred` and `y_true`.\n",
    "\n",
    "    It is particuarly tricky to perform batching in GCN.\n",
    "    As you can see, here the forward pass is performed on the whole graph\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_pred = model(features, adj)  # Do you notice the difference?\n",
    "    acc = accuracy(y_pred[idx_test], labels[idx_test]) \n",
    "    print(f\"Accuracy: {acc:.5}\")\n",
    "    return acc"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CqGfwBtZ6QIz"
   },
   "source": [
    "gcn = GCN(1433, 50, 7)\n",
    "\n",
    "print(\"Loss before training\")\n",
    "_ = gcn_accuracy(gcn)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visualization"
   ],
   "metadata": {
    "id": "lWVxs8509W8o"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qv_180G6QIz"
   },
   "source": [
    "Cora graph visualization:\n",
    "- Yellow nodes: correct predictions\n",
    "- Blue nodes: wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i6TxtUu_6QI0"
   },
   "source": [
    "correct = get_predictions(gcn(features, adj), labels)\n",
    "fig, layt = plot_graph(adj, correct, title=\"GCN performance before training\", layt=layt)\n",
    "fig\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5-XbzZMw6QI0"
   },
   "source": [
    "opt = optim.Adam(gcn.parameters())\n",
    "\n",
    "losses = []\n",
    "gcn.train()\n",
    "\n",
    "for epoch in trange(1000):\n",
    "    opt.zero_grad()\n",
    "    output = gcn(features, adj)  # compute all outputs, even for the nodes in the test set\n",
    "    loss = F.cross_entropy(output[idx_train], labels[idx_train])  # Train only on the train samples!\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "plot_loss(losses)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GdQ_LJ5C6QI0"
   },
   "source": [
    "print(\"Loss after training\")\n",
    "accgcn = gcn_accuracy(gcn)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E2JstJtl6QI0"
   },
   "source": [
    "correct = get_predictions(gcn(features, adj), labels)\n",
    "fig, layt = plot_graph(adj, correct, title=\"GCN performance after training\", layt=layt)\n",
    "fig"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P9jX4Jfq6QI0"
   },
   "source": [
    "def num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) \n",
    "\n",
    "print('Number of parameters MLP: ', num_params(mlp))\n",
    "print('Number of parameters GCN: ', num_params(gcn))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InGr_HNA6QI0"
   },
   "source": [
    "#### Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "both",
    "id": "209wqv4h6QI1"
   },
   "source": [
    "fig = go.Figure([go.Bar(x=['MLP', 'GCN'], y=[accmlp.item(), accgcn.item()])])\n",
    "fig.update_layout(title='Performance comparison',\n",
    "                  yaxis_title=\"Accuracy [%]\",\n",
    "                  xaxis_title=\"Model type\")\n",
    "fig.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9uKoD7eRULB"
   },
   "source": [
    "### **Credits**\n",
    "\n",
    "- Deep Learning & Applied AI @Sapienza Course material byEmanuele Rodol√†, Luca Moschella, and Antonio Norelli - [Original sources](https://erodola.github.io/DLAI-s2-2021/)\n",
    "- Geometric deep learning [tutorial](https://vistalab-technion.github.io/cs236781/tutorials/tutorial_09/)\n",
    "- Kipf T, Welling M. Semi-Supervised Classification with Graph Convolutional Networks (2016).\n",
    "- Bronstein M. M., et al. (2017) Geometric Deep Learning: Going beyond Euclidean data. IEEE Signal Process Mag 34(4)."
   ]
  }
 ]
}
