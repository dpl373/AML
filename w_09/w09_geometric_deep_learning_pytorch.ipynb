{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "lmQgIVZf0pwm",
    "BE8czlWn0pwn"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4C5Ct9yoZKYa"
   },
   "source": [
    "## Optional task 09.2: Understanding Geometric Deep Learning (GDL)\n",
    "\n",
    "ITU KSADMAL1KU - Advanced Machine Learning for Computer Science 2023\n",
    "\n",
    "by Stefan Heinrich, with material by Kevin Murphy.\n",
    "\n",
    "This notebook is based on material by Emanuele RodolÃ , Luca Moschella, and Antonio Norelli.\n",
    "\n",
    "All info and static material: https://learnit.itu.dk/course/view.php?id=3022225\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "*Note: the notebook includes a god amount of reading material code! You are not supposed to understand it all, but follow the questions from the tasks and the inline hints closely.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5CApnc6X4bFq",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# @title #### import dependencies\n",
    "from __future__ import print_function, division\n",
    "\n",
    "!sudo pip install python-igraph\n",
    "!sudo pip install plotly\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from typing import Mapping, Union, Optional, List\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import igraph\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import trange"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T16:39:51.533229Z",
     "start_time": "2020-05-06T16:39:51.515048Z"
    },
    "id": "2tGN_bJOcfd3",
    "cellView": "form"
   },
   "source": [
    "# @title #### for reproducibility\n",
    "# Whenever we use randomness, we should make sure that our results are still reproducible, so we use fixed random seeds.\n",
    "\n",
    "import random\n",
    "np.random.seed(4)\n",
    "random.seed(4)\n",
    "\n",
    "torch.cuda.manual_seed(4)\n",
    "torch.manual_seed(4)\n",
    "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So99OQumzo3M"
   },
   "source": [
    "### Introduction: Digesting *a priori* the information in the structure "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have already learned how to consider the structure information in a simple case; when the domain $\\Omega$ is the Euclidean $\\mathbb{R}^2$, i.e. when the relations between variables are represented by a 2-dimensional grid, such as the pixels of an image.\n",
    "\n",
    "![immagine griglia 2d](https://raw.githubusercontent.com/lucmos/DLAI-s2-2020-tutorials/master/09/pics/euclidean.PNG) \n",
    "\n",
    "The filters of a convolutional neural network elaborate pixels considering only their neighbors and are applied with the same weights all over the image. In this way **CNNs are digesting *a priori* the structure information**, using the properties of the domain -- the translational invariance and the locality of neighbours -- to **reduce the free parameters of the model**. This leads to a crucial speed-up of the training process and allows larger and more powerful models. \n",
    "\n",
    "CNNs can be naturally extended to general Euclidean domains $\\mathbb{R}^n$, but what can we do when $\\Omega \\neq \\mathbb{R}^n$?\n",
    "\n",
    "In many different fields such as Biology, Physics, Social Sciences and Computer Graphics we have to process signals defined on non-Euclidean domains, such as Graphs $\\Omega = G(\\mathcal{V, E})$ or [Manifolds](https://en.wikipedia.org/wiki/Manifold) $\\Omega = \\mathcal{X}$, where $\\mathcal{V} = \\left\\{1,2,\\dots,n\\right\\}$ are vertices or nodes and $\\mathcal{E}\\subseteq \\mathcal{V}\\times \\mathcal{V}$ are edges or connections.\n",
    "\n",
    "We would like to come out with a solution analoguous to CNNs; digesting *a priori* the structure information to reduce the free parameters proved to be very convenient in a learning setting based on a gradient descent optimization.\n"
   ],
   "metadata": {
    "id": "ywEzhCQP5WOy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Representing non-Euclidean data in an Euclidean memory\n",
    "\n",
    "Working with non-Euclidean domains provides a further challenge in representing the data. \n",
    "\n",
    "In the Euclidean setting, encoding the data in ordered matrices, vectors or tensors is so natural and effective that we do not even think to alternatives, and indeed the computer memory structure is itself Euclidean.\n",
    "\n",
    "In the non-Euclidean setting we have many alternatives, consider for instance a manifold $\\mathcal{X}$. It can be represented by a triangle mesh with vertices, edges and faces $\\mathcal{V,E,F}$, by a n-polygonal mesh, where we admit also non-triangle faces, by a simple point cloud or by a subdivision surface. \n",
    "\n",
    "And even when we have chosen the representation, we still have to come out with a last encoding procedure to store our data in matrices and tensors as required by the Euclidean structure of the physical memory in our computers. Is it always going to be this way? Maybe one day we will have new pieces of hardware structured as graphs, like human brains.\n"
   ],
   "metadata": {
    "id": "szHcT8dR5JJF"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66Eh6oxw0pwX"
   },
   "source": [
    "### Graph Convolutional Networks (GCN or ConvGNN) on the Karate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vP64eSzh0pwg"
   },
   "source": [
    "In this section we will see the GCN as defined in the paper [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907), applied to the [Karate dataset](http://networkrepository.com/soc-karate.php), Code and explanations are adapted from this [tutorial](https://nbviewer.org/github/vistalab-technion/cs236781-tutorials/blob/master/t10/tutorial10-GeometricDL.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39Led3Ue0pwh"
   },
   "source": [
    "#### The Karate dataset\n",
    "\n",
    "The Karate dataset is a very small graph with only 34 nodes, the task we will tackle is node classification.\n",
    "\n",
    "This small dataset will allow us to visualize the graph and the behaviour of the graph convolution.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Zachary%27s_karate_club),\n",
    "\n",
    "> A social network of a karate club was studied by Wayne W. Zachary for a period of three years from 1970 to 1972.\n",
    "> The network captures 34 members of a karate club, documenting links between pairs of members who **interacted outside the club**.\n",
    "> During the study a conflict arose between the **administrator** \"John A\" and **instructor** \"Mr. Hi\" (pseudonyms), which led to the split of the club into two.\n",
    "> \n",
    "> Half of the members formed a new club around Mr. Hi; members from the other part found a new instructor or gave up karate. Based on collected data Zachary correctly assigned all but one member of the club to the groups they actually joined after the split. \n",
    "> \n",
    "\n",
    "That is,\n",
    "\n",
    "- Each node is a member of the club\n",
    "- A link means that two members have interacted outside the club\n",
    "- Every node has an attribute `club` that indicates which of the two communities he joined after the split.\n",
    "\n",
    "Our goal is to detected which members of the splitted club will join either of the two splitted communities. \n",
    "\n",
    "**We will use the labels of only two nodes**, and infer the labels of all the other nodes from the graph structure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kVCP9eFK0pwh"
   },
   "source": [
    "# The karate dataset is built-in in networkx package, that we imported above\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "# Known ids of the instructor, admin and members\n",
    "ID_INSTR = 0    \n",
    "ID_ADMIN = 33\n",
    "ID_MEMBERS = set(G.nodes()) - {ID_ADMIN, ID_INSTR}\n",
    "\n",
    "print(f'{G.name}: {len(G.nodes)} vertices, {len(G.edges)} edges')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "ULOu5fCy0pwi"
   },
   "source": [
    "#@title ##### Visualization\n",
    "# Visualize the Karate Club graph using network x\n",
    "fig, ax = plt.subplots(1,1, figsize=(14,8), dpi=100)\n",
    "pos = nx.spring_layout(G, iterations=500)\n",
    "cmap = cmap=plt.cm.tab10\n",
    "node_colors = [0.4] * G.number_of_nodes()\n",
    "node_colors[ID_INSTR] = 0.\n",
    "node_colors[ID_ADMIN] = 1.\n",
    "node_labels = {i: i for i in ID_MEMBERS}\n",
    "node_labels.update({i: l for i,l in zip([ID_ADMIN, ID_INSTR],['A','I'])})\n",
    "nx.draw_networkx(G, \n",
    "                 pos, \n",
    "                 node_color=node_colors, \n",
    "                 width=.25,\n",
    "                 labels=node_labels, \n",
    "                 ax=ax, \n",
    "                 cmap=cmap);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYYCIqa20pwi"
   },
   "source": [
    "Looking at the graph, it looks like there are two communities around the instructor and the administrator. \n",
    "\n",
    "It is intuitive to assume the members close to the instructor will join the community of the instructor when the club will split, and the same with the administrator. Even though, for some nodes it is not obvious even for us to predict which cummunity they will join.\n",
    "\n",
    "The graph neural networks are able to *learn* effectively from the *structure* of the graph, and make predictions according to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JAvhp2L0pwi"
   },
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7MCEQH40pwi"
   },
   "source": [
    "Thus, to recap, we're going to use only two labels and perform the prediction on the other nodes looking only at the graph structure.\n",
    "\n",
    "Each vertex has a `club` attribute to represent the post-split community he joined, we will train using the `club` labels for the Instructor and the Administrator only.\n",
    "\n",
    "For the other nodes we will just use a one-hot encoding, that are *not informative* on which community that node will join. Thus the model will have to learn everything from the structure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "Prn5Y-dE0pwi"
   },
   "source": [
    "# Input featuers (no information on nodes):\n",
    "X = torch.eye(G.number_of_nodes())\n",
    "\n",
    "# Create ground-truth labels\n",
    "# - Assign the label \"0\" to the \"Mr. Hi\" community\n",
    "# - Assign the label \"1\" to the \"Officer\" community\n",
    "labels = [int(not d['club']=='Mr. Hi') for _, d in G.nodes().data()]\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Let's check the nodes metadata\n",
    "for (node_id, node_data), label_id in zip(G.nodes().data(), labels):\n",
    "    print(f'Node id: {node_id},\\tClub: {node_data[\"club\"]},\\t\\tLabel: {label_id.item()}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_oJm5Eq0pwj"
   },
   "source": [
    "##### Adjacency matrices as operators\n",
    "\n",
    "We can see *node-to-node* adjacency matrices as *operators* when applied to functions over the vertices of a graph.\n",
    "\n",
    "For example, $\\mathbf{g} = \\mathbf{Af}$ yields a new *node-based* function $g$ defined as:\n",
    "\n",
    "$$\n",
    "g(v_i) = \\sum_{e_{ij} \\in E} f(v_j)\n",
    "$$\n",
    "\n",
    "On this observation, one can construct new adjacency-based operators such as $\\mathbf{I-A}$:\n",
    "\n",
    "$$\n",
    "g(v_i) = f(v_i) - \\sum_{e_{ij} \\in E} f(v_j)\n",
    "$$\n",
    "\n",
    "Or such as... the Laplacian operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qxb3_T3O0pwk"
   },
   "source": [
    "-------------------------------------------------------------------------------\n",
    "#### **Task**\n",
    "Understand how we represent the task for the GCN and in particular, the node-to-node operators.\n",
    "\n",
    "  - What does the $k$-power of the adjacency matrix $A$ represent?\n",
    "  - I.e. what does the element $(A^k)_{ij}$ represent? \n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gh-aXJ4k0pwk"
   },
   "source": [
    "#### Graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or5TsUho0pwk"
   },
   "source": [
    "##### Laplacian computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGswpKZz0pwk"
   },
   "source": [
    "The Graph Laplacian is defined as follows:\n",
    "\n",
    "$$\n",
    "L_{ij} =\n",
    "\\begin{cases}\n",
    " D_{ii} & \\text{if } i = j \\\\\n",
    " -1 & \\text{if } i \\neq j \\text{ and } e_{ij} \\in E \\\\\n",
    " 0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $D$ is the diagonal matrix that contains the degree of the vertices $v_i$ along the diagonal.\n",
    "\n",
    "Thus, the Laplacian can be written in matrix notation as:\n",
    "\n",
    "$$\n",
    "L = D - A\n",
    "$$\n",
    "\n",
    "The Laplacian can be seen as a vertex-based operator, it is a function $f: V \\to \\mathcal{R}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Tt4SXX0P0pwk"
   },
   "source": [
    "# Adjacency matrix, binary\n",
    "A = nx.adjacency_matrix(G, weight=None)\n",
    "A = np.array(A.todense())\n",
    "A"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KeBDMmlF0pwk"
   },
   "source": [
    "# Degree matrix\n",
    "dii = np.sum(A, axis=1, keepdims=False)  # sum the columns of the adj\n",
    "D = np.diag(dii)\n",
    "D"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4pdjYhNc0pwl"
   },
   "source": [
    "# Laplacian\n",
    "L = D - A\n",
    "L"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1b-oBRcY0pwl"
   },
   "source": [
    "##### The Laplacian matrix has some interesting properties\n",
    "\n",
    "The Laplacian $L$ of an undirected graph has many interesting properties, some of them are:\n",
    "\n",
    "- It is symmetric and has the same zero pattern of the adjacency matrix\n",
    "- It is [positive-semidefinite](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix)\n",
    "- Every row sum and column sum of $L$ is zero.\n",
    "- The trace of the Laplacian Matrix $L$ is equal to $2m$ where $m$ is the number of edges of the considered graph. (This is straighforward to prove: the sum of the degrees of the vertices is twice the number of the edges. Why? Each edge connects exactly two vertices. Thus, it is counted twice in the sum of degrees, once for each vertex it connects.)\n",
    "- For a graph with multiple connected components, $L$ is a block diagonal matrix (possibly after reordering the vertices), where each block is the Laplacian matrix of each component."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a_7OSskY0pwl"
   },
   "source": [
    "# Symmetric\n",
    "(L.transpose() == L).all()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iNSBcun80pwl"
   },
   "source": [
    "# Sum of degrees\n",
    "np.trace(L) == 2 * G.number_of_edges()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C1BU6VS80pwl"
   },
   "source": [
    "# Sum of colums/rows is zero\n",
    "print(np.sum(L, axis=1))\n",
    "print(np.sum(L, axis=0))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKuYZiCN0pwl"
   },
   "source": [
    "##### The Laplacian spectrum\n",
    "\n",
    "As you already saw, the eigenfunctions of the Laplacian operator are a generalization of the Fourier basis, which we can use as a basis to represent functions over our domain.\n",
    "\n",
    "Even the spectrum, i.e. the eigenvalues of the Laplacian, have many interesting properties. Some of them are:\n",
    "\n",
    "- The first eigenvalue is always zero\n",
    "- The number of connected components in the graph is the multiplicity of the zero eigenvalue\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K9GyjMAe0pwl"
   },
   "source": [
    "# Compute the eigevanlues and eigenvector\n",
    "w, Phi = np.linalg.eigh(L)\n",
    "\n",
    "plt.plot(w); plt.xlabel(r'$\\lambda$'); plt.title('Spectrum');"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TDvx7uc0pwl"
   },
   "source": [
    "##### Laplacian basis\n",
    "\n",
    "Let's visualize the Laplacian basis of the Karate graph, we expect an oscillatory behaviour similar to the Fourier basis on signals. \n",
    "\n",
    "It will be a bit difficult to see, since there are few nodes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lB4x6BHJ0pwm"
   },
   "source": [
    "#@title ##### Visualization \n",
    "\n",
    "# Plot Fourier basis\n",
    "fig, ax = plt.subplots(4, 4, figsize=(8,6), dpi=150)\n",
    "ax = ax.reshape(-1)\n",
    "vmin, vmax = np.min(Phi), np.max(Phi)\n",
    "for i in range(len(ax)):\n",
    "    nc = Phi[:,i]\n",
    "    nx.draw_networkx(G, pos, node_color=nc, with_labels=False, node_size=15, ax=ax[i], width=0.25, cmap=plt.cm.magma, vmin=vmin, vmax=vmax)\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(rf'$\\lambda_{{{i}}}={w[i]:.2f}$',fontdict=dict(fontsize=8))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a68YkaB0pwm"
   },
   "source": [
    "##### Normalized graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6BC8VqQ0pwm"
   },
   "source": [
    "Following the paper, we will use a normalized flavour of the Laplacian, defined as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Delta} = \\mathbf{D}^{-1/2}\\tilde{\\mathbf{A}}\\mathbf{D}^{-1/2}.\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\tilde{\\mathbf{A}}=\\mathbf{A}+\\mathbf{I}$\n",
    "- $\\mathbf{A}$ is the adjacency matrix \n",
    "- $\\mathbf{D}$ is the degree matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMAiEC5B0pwm"
   },
   "source": [
    "Intuitivelly: $\\Delta$ is the adjacency matrix with enforced self-loops, normalized by the geometric mean of the degrees of the two nodes:\n",
    "$$\n",
    "\\Delta_{ij} = \\frac{\\tilde{A}_{ij}}{\\sqrt{D_{ii} D_{jj}}}.\n",
    "$$\n",
    "\n",
    "The self-loops allow the output features of a node to also depend on its input features, not just on its neighbors. Without the self-loop each node would consider its neighbours' features, but not its own!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zcAaBqBe0pwm"
   },
   "source": [
    "# Adjacency matrix\n",
    "A = nx.adjacency_matrix(G, weight=None)\n",
    "A = np.array(A.todense())\n",
    "I = np.eye(A.shape[0])\n",
    "A = A + I\n",
    "\n",
    "# Degree matrix (only the diagonal)\n",
    "dii = np.sum(A, axis=1, keepdims=False)\n",
    "#D = np.diag(dii)\n",
    "\n",
    "# Normalized Laplacian\n",
    "D_inv_h = np.diag(dii**(-0.5))\n",
    "L =  D_inv_h @ A @ D_inv_h"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "-------------------------------------------------------------------------------\n",
    "#### **Task**\n",
    "Now understand how to build the GCN model by using a Laplacian as filter.\n",
    "\n",
    "  - What would our matrices $Z$ (feature maps) and $W$ (weights) represent?  \n",
    "-------------------------------------------------------------------------------"
   ],
   "metadata": {
    "id": "ttDka1rLDmel"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6Jpxznw0pwm"
   },
   "source": [
    "#### Model\n",
    "\n",
    "In the paper (see above) the authors choose to use $\\tau_\\alpha(\\lambda_i) = \\sum_{k=1}^{q}\\alpha_k\\lambda_i^k \\;$, thus the convolutional filter $\\mathbf{W}$ becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{W} = \\mathbf{\\Phi}\\mathbf{\\hat W}\\mathbf{\\Phi}^\\top\n",
    "&= \n",
    "\\mathbf{\\Phi}\n",
    "\\begin{pmatrix}\n",
    "\\sum_{k=1}^{q}\\alpha_k\\lambda_1^k &  &  \\\\\n",
    " & \\ddots &  \\\\\n",
    " &  & \\sum_{k=1}^{q}\\alpha_k\\lambda_n^k \\\\\n",
    "\\end{pmatrix}\n",
    "\\mathbf{\\Phi}^\\top\n",
    "=\n",
    "\\sum_{k=1}^{q}\\alpha_k\n",
    "\\mathbf{\\Phi}\n",
    "\\begin{pmatrix}\n",
    "\\lambda_1^k &  &  \\\\\n",
    " & \\ddots &  \\\\\n",
    " &  & \\lambda_n^k \\\\\n",
    "\\end{pmatrix}\n",
    "\\mathbf{\\Phi}^\\top \\\\\n",
    "&=  \\sum_{k=1}^{q}\\alpha_k \\mathbf{\\Phi} \\mathbf{\\Lambda}^k \\mathbf{\\Phi}^\\top\n",
    "= \\sum_{k=1}^{q}\\alpha_k \\mathbf{\\Delta}^k.\n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU4dzw_90pwm"
   },
   "source": [
    "With this choice of $\\tau_\\alpha$ it is not necessary to compute the eigendecomposition of the Laplacian $\\mathbf{\\Phi}\\mathbf{\\Lambda}\\mathbf{\\Phi}^\\top$, we can use directly the $k$-powers of $\\mathbf{\\Delta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjEkvoBP0pwm"
   },
   "source": [
    "Our model will have two GCN layers.\n",
    "Each layer takes a tensor containing $C_\\text{in}$ features for each node,\n",
    "and returns a tensor containing $C_\\text{out}$ features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yV5jNUJV0pwm"
   },
   "source": [
    "import torch.nn as nn\n",
    "from typing import List\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 graph_L: torch.Tensor, \n",
    "                 in_features: int, \n",
    "                 out_features: int, \n",
    "                 max_deg: int = 1\n",
    "        ):\n",
    "        \"\"\"\n",
    "        :param graph_L: the normalized graph laplacian. It is all the information we need to know about the graph\n",
    "        :param in_features: the number of input features for each node\n",
    "        :param out_features: the number of output features for each node\n",
    "        :param max_deg: how many power of the laplacian to consider, i.e. the q in the spacial formula\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Each FC is like the alpha_k matrix, with the last one including bias\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for i in range(max_deg - 1):\n",
    "            self.fc_layers.append(nn.Linear(in_features, out_features, bias=False))     # q - 1 layers without bias\n",
    "        self.fc_layers.append(nn.Linear(in_features, out_features, bias=True))          # last one with bias\n",
    "        \n",
    "        # Pre-calculate beta_k(L) for every key\n",
    "        self.laplacians = self.calc_laplacian_functions(graph_L, max_deg)\n",
    "        \n",
    "    def calc_laplacian_functions(self, \n",
    "                                 L: torch.Tensor, \n",
    "                                 max_deg: int\n",
    "        ) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute all the powers of L from 1 to max_deg\n",
    "\n",
    "        :param L: a square matrix\n",
    "        :param max_deg: number of powers to compute\n",
    "\n",
    "        :returns: a list of tensors, where the element i is L^{i+1} (i.e. start counting from 1)\n",
    "        \"\"\"\n",
    "        res = [L]\n",
    "        for _ in range(max_deg-1):\n",
    "            res.append(torch.mm(res[-1], L))\n",
    "        return res\n",
    "        \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform one forward step of graph convolution\n",
    "\n",
    "        :params X: input features maps [vertices, in_features]\n",
    "        :returns: output features maps [vertices, out_features]\n",
    "        \"\"\"\n",
    "        Z = torch.tensor(0.)\n",
    "        for k, fc in enumerate(self.fc_layers):\n",
    "            L = self.laplacians[k]\n",
    "            LX = torch.mm(L, X)\n",
    "            Z = fc(LX) + Z\n",
    "        \n",
    "        return torch.relu(Z)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmQgIVZf0pwm"
   },
   "source": [
    "#### **Optional expert tasks: bug in sight!**\n",
    "\n",
    "If you try to serialize (e.g. save on disk) a trained instance of the previous model with `torch.save`, and then load it again with `torch.load`... it will not work. \n",
    "\n",
    "* Can you see why? How would you solve this problem?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2-H_JagM0pwn"
   },
   "source": [
    "#@title ##### Solution ð\n",
    "\n",
    "# The laplacians are not saved!\n",
    "# Check out the Buffers in pytorch to save them:\n",
    "# https://pytorch.org/docs/stable/nn.html?highlight=register_buffer#torch.nn.Module.register_buffer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GYZ62Rsn0pwn"
   },
   "source": [
    "in_features, out_features = X.shape[1], 2\n",
    "graph_L = torch.tensor(L, dtype=torch.float)\n",
    "max_deg = 2\n",
    "hidden_dim = 5\n",
    "\n",
    "# Stack two GCN layers as our model\n",
    "gcn2 = nn.Sequential(\n",
    "    GCNLayer(graph_L, in_features, hidden_dim, max_deg),\n",
    "    GCNLayer(graph_L, hidden_dim, out_features, max_deg),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "gcn2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BE8czlWn0pwn"
   },
   "source": [
    "#### Training\n",
    "\n",
    "We'll train a simple classification task, with the nuance that only the Instructor and Administrator labels are used."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RS3_uKwU0pwn"
   },
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "\n",
    "def train_node_classifier(model, optimizer, X, y, epochs=60, print_every=10):\n",
    "    y_pred_epochs = []\n",
    "    for epoch in range(epochs+1):\n",
    "        y_pred = model(X)  # Compute on all the graph\n",
    "        y_pred_epochs.append(y_pred.detach())\n",
    "\n",
    "        # Semi-supervised: only use labels of the Instructor and Admin nodes\n",
    "        labelled_idx = [ID_ADMIN, ID_INSTR]\n",
    "        loss = F.nll_loss(y_pred[labelled_idx], y[labelled_idx])  # loss on only two nodes\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f'Epoch {epoch:2d}, loss={loss.item():.5f}')\n",
    "    return y_pred_epochs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8Mw78S3d0pwn"
   },
   "source": [
    "optimizer = torch.optim.Adam(gcn2.parameters(), lr=0.01)\n",
    "\n",
    "y_pred_epochs = train_node_classifier(gcn2, optimizer, X, labels)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo9NFn1o0pwn"
   },
   "source": [
    "Since our loss is calculated based on two samples only, it's not a good criterion of overall classification accuracy.\n",
    "\n",
    "Let's look at the the accuracy over all nodes:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ANDo9oD_0pwn"
   },
   "source": [
    "y_pred = torch.argmax(gcn2(X), dim=1).detach().numpy()\n",
    "y = labels.numpy()\n",
    "print(classification_report(y, y_pred, target_names=['I','A']))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cqv_KoSU0pwn"
   },
   "source": [
    "#### Compare with MLP\n",
    "\n",
    "Lets see what we get when we use a regular MLP on the same task.\n",
    "\n",
    "Remember, we're only training using 2/34 labeled samples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "fdITug4o0pwo"
   },
   "source": [
    "mlp = nn.Sequential(\n",
    "    nn.Linear(in_features, hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_dim, out_features),\n",
    "    nn.ReLU(),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)\n",
    "_ = train_node_classifier(mlp, optimizer, X, labels, epochs=2000, print_every=500)\n",
    "\n",
    "print(classification_report(labels.numpy(), torch.argmax(mlp(X), dim=1).detach().numpy(), target_names=['I','A']))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7RusjqB0pwo"
   },
   "source": [
    "-------------------------------------------------------------------------------\n",
    "#### **Task**\n",
    "As expected, the MLP can't learn much about the nodes that we didn't train on.\n",
    "\n",
    "  - Why was the GCN able to achieve this? \n",
    "\n",
    "   *Hint:* Recall that due to the multiplication with the Laplacian, the node embeddings calculated by the GCN at each layer combine the previous layer features from neighbouring nodes.\n",
    "\n",
    "   When we back-propagated from our two labelled nodes, we also updated the model parameters to produce more meaningful embedding for their neighbours!  \n",
    "\n",
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8m_iSD9s0pwo"
   },
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fcJTtNFO0pwo"
   },
   "source": [
    "# @title ##### Visualization - Ignore the methods in this section. The code is not necessary to unterstand for the tasks. They are just used for some nicer visualisations.\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import math\n",
    "def animate_classification(G, y_pred_epochs):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6), dpi=150)\n",
    "    \n",
    "    def draw(epoch_idx):\n",
    "        pos = {}\n",
    "        colors, sizes = [],[]\n",
    "        for v in range(G.number_of_nodes()):\n",
    "            pos[v] = y_pred_epochs[epoch_idx][v].numpy()\n",
    "            y_pred_v = np.argmax(pos[v])\n",
    "            y_v = labels[v]\n",
    "            if y_pred_v == y_v: colors.append(y_v)\n",
    "            else: colors.append(0.4) # wrong prediction\n",
    "            sizes.append((math.exp(pos[v][y_v]))*300) # size is proba of correct label\n",
    "        ax.cla()\n",
    "        ax.set_title(f'Epoch {epoch_idx}')\n",
    "        ax.set_xlabel('Log Softmax Score 1')\n",
    "        ax.set_ylabel('Log Softmax Score 2')\n",
    "        nx.draw_networkx(G, \n",
    "                         pos, \n",
    "                         node_color=colors, \n",
    "                         labels=node_labels, \n",
    "                         ax=ax, \n",
    "                         cmap=cmap, \n",
    "                         node_size=sizes, \n",
    "                         width=0.1)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, draw, frames=len(y_pred_epochs), interval=150)\n",
    "    html = HTML(anim.to_html5_video())\n",
    "    plt.close()\n",
    "    return html"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4Tte8RS90pwo"
   },
   "source": [
    "# The coordinates are the log_softmax scores\n",
    "animate_classification(G, y_pred_epochs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9uKoD7eRULB"
   },
   "source": [
    "### **Credits**\n",
    "\n",
    "- Deep Learning & Applied AI @Sapienza Course material byEmanuele RodolÃ , Luca Moschella, and Antonio Norelli - [Original sources](https://erodola.github.io/DLAI-s2-2021/)\n",
    "- Geometric deep learning [tutorial](https://vistalab-technion.github.io/cs236781/tutorials/tutorial_09/)\n",
    "- Kipf T, Welling M. Semi-Supervised Classification with Graph Convolutional Networks (2016).\n",
    "- Bronstein M. M., et al. (2017) Geometric Deep Learning: Going beyond Euclidean data. IEEE Signal Process Mag 34(4)."
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "irKb-T9s9OoX"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
