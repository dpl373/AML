
# Exercise week 9

This is your 9th mandatory exercise which must be completed until next week, i.e. 06.11.2023, 10:00. By that time please fill out the checklist on the [learnit page](https://learnit.itu.dk/course/view.php?id=3022225) to indicate which tasks you volunteer to present. 
You are very welcome to present incomplete solutions and describe what challenges you faced.
Please note that if not stated otherwise the programming exercises should be implemented by you, not by using a library. The intention is that you learn the transition from equations to code. Later in the course you are welcome to use programming frameworks.


## Task 1: Generative Adversarial Networks
Training a GAN can be a challenge. In this exercise we invite you to try it for yourself, and see if you can succeed. 
We provide a sample notebook, which you can use as a starting point.
You probably need to download some data first. Please note: In the provided sample notebook *w09_gan.ipynb*, we use [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html). You are free to use another dataset, [MNIST](http://yann.lecun.com/exdb/mnist/) or [celebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). You might find it helpful to read more about the [pytorch dataloader](https://pytorch.org/vision/0.16/datasets.html). 

Note: Remember to change the data path in the notebook.



<ol type ="a">
  <li>Complete the notebook by adding the implementation for the generator and discriminator.</li>
  <li>Which parts of the notebook have to be changed if you want to use another dataset? How do you need to change them? </li>
  <li>Please document your findings when executing the notebook. Did your training converge? How did the intermediate generated images look like?</li>
  <li> Name several things which can be changed to receive a different result after training.</li>  
  <li> Choose something to change, e.g. setting, preprocessing or parameters, and run the notebook (at least) 2 more times with different configurations. Save your results. Did the training converge? How do the generated images look like? How do the three different models perform in comparison?</li> 
</ol>

Non-mandatory: Comment on your suggestions to change the code. 


## Task 2: Understanding Geometric Deep Learning (GDL)

In this task, we explore convolution in graph neural networks, in particular Graph Convolution Networks (GCNs), via *w09_geometric_deep_learning_pytorch.ipynb* (or in [Colab](https://colab.research.google.com/drive/1kxRyHdNmXyw44jR_rmmjsDM3AB9RuUMd)).

The goal is first, to get to know about extending convolutional filters from images (grids) to the polynomial case (graphs), and second, to experience the GCN implementation in torch.

a. Understand how we represent the task for the GCN and in particular, the node-to-node operators.

   - What does the $k$-power of the adjacency matrix $A$ represent?
   - I.e. what does the element $(A^k)_{ij}$ represent? 

b. Now understand how to build the GCN model by using a Laplacian as filter.

   - What would our matrices $Z$ (feature maps) and $W$ (weights) represent? 

c. As expected, the MLP can't learn much about the nodes that we didn't train on.

   - Why was the GCN able to achieve this? 

      *Hint:* Recall that due to the multiplication with the Laplacian, the node embeddings calculated by the GCN at each layer combine the previous layer features from neighbouring nodes.

      When we back-propagated from our two labelled nodes, we also updated the model parameters to produce more meaningful embedding for their neighbours!  



